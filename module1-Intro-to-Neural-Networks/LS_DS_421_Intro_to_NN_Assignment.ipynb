{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "The input layer is made up of artificial input 'neurons', and brings in the data for processing in the subsequent layers.\n",
    "### Hidden Layer:\n",
    "Hidden layers are the layers between the input and output that handle the actual computations you are trying to do, and pass their new data onto the next hidden layer, until the data is ready to be output.\n",
    "### Output Layer:\n",
    "An output layer is basically just the result of your computations done by your hidden layers, on the data from the input layer.\n",
    "### Neuron:\n",
    "A neuron in the context if Artificial NNs is a mathematical function that models the function of a biological neuron\n",
    "### Weight:\n",
    "weight is the parameter that actually changes the input data while moving through the hidden layers\n",
    "### Activation Function:\n",
    "an activation function basically just defines the output of a node based on the input or set of inputs\n",
    "### Node Map:\n",
    "Node maps are a visual diagram of the topology, or architecture, of an artificial neural network. They are usually color coded and help us understand the architectural differences between different kinds of neural networks\n",
    "### Perceptron:\n",
    " A perceptron does certain computations to detect features and/or business intelligence in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### You start with an input. I like the example from 3blue1brown about handwritten numbers, so I'll use that. \n",
    "In this example, the input data will be a handwritten '9', in a 28x28 pixel format. the input image first assigns weights to the input data, row by row, then the bias is what tells the network how high the weighted sums need to be in order for it to be meaningfully active. In this case, the weights will tell you where edges are on each layer, then the bias will tell the network when an edge is actually meaningful. This will create a new neuron for every row of the image, each with a weight and bias indicating how meaningful it is. These new neurons will make up our first hidden layer. Now the network will look through each of those edges, and decide if any can be put together to form a pattern. in the example, the patterns we would want to find for a '9' are a circle on the top half, and a vertical line extending down from the right side of the circle. So the second hidden layer with find all of the different possible patterns. Finally, the output will give the last hidden layer a certain number of options, and the Network will decide which is most likely, and give you an output. In this example, the activation function would go up from 0-9 looking for the right patterns,(for example, would see the circle at the top of an eight, then see that our input doesnt have a second circle below it, so it would move on to 9), confirm the patterns, then give you the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "# instantiate our x variable; matrix of x1, x2, and one with all 1s for the bias\n",
    "inputs = np.array([\n",
    "    [0,0,1],\n",
    "    [1,0,1],\n",
    "    [0,1,1],\n",
    "    [1,1,1]\n",
    "])\n",
    "\n",
    "correct_outputs = [[1], [1], [1], [0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.51385916],\n",
       "       [-0.37708675],\n",
       "       [-0.08140275]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize random weights for our 3 inputs(need weight for each input)\n",
    "weights = 2 * np.random.random((3,1)) - 1\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08140275],\n",
       "       [-0.59526191],\n",
       "       [-0.4584895 ],\n",
       "       [-0.97234866]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate weighted sum of inputs\n",
    "weighted_sum = np.dot(inputs, weights)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47966054],\n",
       "       [0.35542844],\n",
       "       [0.38734422],\n",
       "       [0.27441261]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activated value for the end of 1 training epoch\n",
    "activated_output = sigmoid(weighted_sum)\n",
    "activated_output # true values are [1,1,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error\n",
    "error = correct_outputs - activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1228804 ],\n",
       "       [ 0.15615889],\n",
       "       [ 0.1475596 ],\n",
       "       [-0.0673277 ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient descent/backprop-- magic for now, will learn in LS_DS_422\n",
    "adjusted = error * sigmoid_derivative(activated_output)\n",
    "adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42502797],\n",
       "       [-0.29685485],\n",
       "       [ 0.27786845]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update weights\n",
    "weights += np.dot(inputs.T, adjusted)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[-11.84023451]\n",
      " [-11.84023451]\n",
      " [ 17.80921618]]\n",
      "Output after training\n",
      "[[0.99999998]\n",
      " [0.99744942]\n",
      " [0.99744942]\n",
      " [0.00281169]]\n"
     ]
    }
   ],
   "source": [
    "# Steps we've already done: \n",
    "# 1. Randomly Initialized Weights already. Those are in memory as `weights`\n",
    "# 2. We've already got input data & correct_outputs\n",
    "\n",
    "\n",
    "# Update our weights 10,000 times - (fingers crossed that this process reduces error)\n",
    "for iteration in range(10000):\n",
    "    \n",
    "    # Weighted sum of inputs / weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Cac error\n",
    "    error = correct_outputs - activated_output\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    # Update the Weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Outcome'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.columns[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "# features\n",
    "feats = list(diabetes)[:-1]\n",
    "# split up into train and test data\n",
    "X = diabetes[feats]\n",
    "normalized = Normalizer().fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "        sx = sigmoid(x)\n",
    "        return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = sigmoid(x)\n",
    "        return sx * (1-sx)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Randomly Initialize Weights\n",
    "        # weights\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        # Number of misclassifications\n",
    "        self.errors = []  # Number of misclassifications\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            # weighted sum of inputs\n",
    "            weighted_sum = np.dot(X, self.weights)    \n",
    "            # Activate!\n",
    "            activated_output = sigmoid(weighted_sum)    \n",
    "            # Cac error\n",
    "            error = y - activated_output    \n",
    "            adjustments = error * sigmoid_derivative(activated_output)   \n",
    "            # Update the Weights\n",
    "            self.weights += np.dot(X.T, adjustments)\n",
    "        return self.weights\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        calcs = np.dot(X, self.weights)\n",
    "        return np.where(calcs >= .5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Normalizer().fit_transform(diabetes[feats])\n",
    "y = diabetes['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30.0738127 , -14.71242511, -93.98387466, -13.61855156,\n",
       "       -16.99418773,  -0.87510791,   1.04191842,  -5.00087061])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron(150)\n",
    "weights = perceptron.fit(X, y)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = perceptron.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pn = Perceptron()\n",
    "fit = pn.fit(X, y)\n",
    "plt.plot(range(1, len(pn.errors) + 1), pn.errors, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
