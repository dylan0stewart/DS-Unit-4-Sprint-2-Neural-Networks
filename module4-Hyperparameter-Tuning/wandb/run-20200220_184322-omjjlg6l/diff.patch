diff --git a/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb b/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
index 307b203..414d5c1 100644
--- a/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
+++ b/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
@@ -242,9 +242,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.7.4"
   }
  },
  "nbformat": 4,
- "nbformat_minor": 2
+ "nbformat_minor": 4
 }
diff --git a/module2-backpropagation/LS_DS_422_Gradient_Descent_Backprop_Lecture.ipynb b/module2-backpropagation/LS_DS_422_Gradient_Descent_Backprop_Lecture.ipynb
index 17ea4bd..635b316 100644
--- a/module2-backpropagation/LS_DS_422_Gradient_Descent_Backprop_Lecture.ipynb
+++ b/module2-backpropagation/LS_DS_422_Gradient_Descent_Backprop_Lecture.ipynb
@@ -939,9 +939,9 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "U4-S2-NNF (Python 3.7)",
+   "display_name": "Python 3",
    "language": "python",
-   "name": "u4-s2-nnf"
+   "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
@@ -953,7 +953,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.7.4"
   }
  },
  "nbformat": 4,
diff --git a/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb b/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb
index 2457723..4b9d616 100644
--- a/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb
+++ b/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb
@@ -32,15 +32,184 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 38,
    "metadata": {
     "colab": {},
     "colab_type": "code",
     "id": "8NLTAR87uYJ-"
    },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "((404,), (13,), (102, 13), (102,))"
+      ]
+     },
+     "execution_count": 38,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "#imports\n",
+    "from tensorflow import keras \n",
+    "from tensorflow.keras.datasets import boston_housing\n",
+    "from tensorflow.keras.models import Sequential\n",
+    "from tensorflow.keras.layers import Dense, Dropout\n",
+    "\n",
+    "import numpy as np\n",
+    "\n",
+    "\n",
+    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data() # load data in, already seperated by features/target\n",
+    "y_train.shape, X_train[0].shape, X_test.shape, y_test.shape"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 37,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "((5252, 1), (404, 1), (1326, 1), (102, 1))"
+      ]
+     },
+     "execution_count": 37,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "from sklearn.preprocessing import Normalizer\n",
+    "\n",
+    "norm = Normalizer()\n",
+    "\n",
+    "X_train = norm.fit_transform(X_train.reshape(-1,1))\n",
+    "X_test = norm.fit_transform(X_test.reshape(-1,1))\n",
+    "y_train = norm.fit_transform(y_train.reshape(-1,1))\n",
+    "y_test = norm.fit_transform(y_test.reshape(-1,1))\n",
+    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 33,
+   "metadata": {},
    "outputs": [],
    "source": [
-    "##### Your Code Here #####"
+    "X_train = X_train.reshape(404, 13)\n",
+    "X_test = X_test.reshape(102, 13)\n",
+    "X_train = X_train.astype('float32')\n",
+    "X_test = X_test.astype('float32')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 36,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "((404, 13), (404,), (102, 13), (102,))"
+      ]
+     },
+     "execution_count": 36,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 42,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from tensorflow.keras.utils import to_categorical\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 53,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Model: \"sequential_9\"\n",
+      "_________________________________________________________________\n",
+      "Layer (type)                 Output Shape              Param #   \n",
+      "=================================================================\n",
+      "dense_35 (Dense)             (None, 8)                 112       \n",
+      "_________________________________________________________________\n",
+      "dense_36 (Dense)             (None, 8)                 72        \n",
+      "_________________________________________________________________\n",
+      "dense_37 (Dense)             (None, 1)                 9         \n",
+      "=================================================================\n",
+      "Total params: 193\n",
+      "Trainable params: 193\n",
+      "Non-trainable params: 0\n",
+      "_________________________________________________________________\n"
+     ]
+    }
+   ],
+   "source": [
+    "model = Sequential()\n",
+    "\n",
+    "# Hidden\n",
+    "model.add(Dense(8, input_dim=13, activation='relu'))\n",
+    "model.add(Dense(8, activation='relu'))\n",
+    "# Output Layer\n",
+    "model.add(Dense(1, activation='softmax'))\n",
+    "\n",
+    "model.compile(loss='mean_squared_error',\n",
+    "                    optimizer='adam')\n",
+    "model.summary()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 54,
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "ValueError",
+     "evalue": "A target array with shape (404, 51) was passed for an output of shape (None, 1) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output.",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
+      "\u001b[1;32m<ipython-input-54-093f73d82fad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
+      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
+      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
+      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2487\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2489\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m       sample_weights, _, _ = training_utils.handle_partial_sample_weights(\n",
+      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    808\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0;32m    809\u001b[0m                            \u001b[1;34m' was passed for an output of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m                            \u001b[1;34m' while using as loss `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m                            \u001b[1;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m                            'as the output.')\n",
+      "\u001b[1;31mValueError\u001b[0m: A target array with shape (404, 51) was passed for an output of shape (None, 1) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output."
+     ]
+    }
+   ],
+   "source": [
+    "model.fit(X_train,y_train, epochs=250)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "history = mnist_model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=False)\n",
+    "scores = mnist_model.evaluate(X_test, y_test)\n",
+    "print(f'{mnist_model.metrics_names[1]}: {scores[1]*100}')"
    ]
   },
   {
@@ -110,9 +279,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.7.4"
   }
  },
  "nbformat": 4,
- "nbformat_minor": 2
+ "nbformat_minor": 4
 }
diff --git a/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb b/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb
index abc92b5..66ead93 100644
--- a/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb
+++ b/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb
@@ -75,7 +75,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 52,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -93,7 +93,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 53,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -110,24 +110,24 @@
      "text": [
       "Train on 4 samples\n",
       "Epoch 1/5\n",
-      "4/4 [==============================] - 1s 184ms/sample - loss: 0.7531 - accuracy: 0.2500\n",
+      "4/4 [==============================] - 0s 58ms/sample - loss: 0.7340 - accuracy: 0.5000\n",
       "Epoch 2/5\n",
-      "4/4 [==============================] - 0s 1ms/sample - loss: 0.7527 - accuracy: 0.5000\n",
+      "4/4 [==============================] - 0s 250us/sample - loss: 0.7336 - accuracy: 0.7500\n",
       "Epoch 3/5\n",
-      "4/4 [==============================] - 0s 833us/sample - loss: 0.7524 - accuracy: 0.5000\n",
+      "4/4 [==============================] - 0s 752us/sample - loss: 0.7332 - accuracy: 0.7500\n",
       "Epoch 4/5\n",
-      "4/4 [==============================] - 0s 760us/sample - loss: 0.7520 - accuracy: 0.5000\n",
+      "4/4 [==============================] - 0s 498us/sample - loss: 0.7328 - accuracy: 0.7500\n",
       "Epoch 5/5\n",
-      "4/4 [==============================] - 0s 805us/sample - loss: 0.7517 - accuracy: 0.5000\n"
+      "4/4 [==============================] - 0s 249us/sample - loss: 0.7325 - accuracy: 0.7500\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7fb4cc4cdeb8>"
+       "<tensorflow.python.keras.callbacks.History at 0x186cc7ecdc8>"
       ]
      },
-     "execution_count": 2,
+     "execution_count": 53,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -145,7 +145,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 54,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -160,8 +160,8 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "4/1 [========================================================================================================================] - 0s 21ms/sample - loss: 0.7514 - accuracy: 0.5000\n",
-      "accuracy: 50.0\n"
+      "4/4 [==============================] - 0s 13ms/sample - loss: 0.7321 - accuracy: 0.7500\n",
+      "accuracy: 75.0\n"
      ]
     }
    ],
@@ -205,7 +205,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 55,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -240,7 +240,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 56,
    "metadata": {},
    "outputs": [
     {
@@ -277,7 +277,7 @@
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
-       "      <td>0</td>\n",
+       "      <th>0</th>\n",
        "      <td>6</td>\n",
        "      <td>148</td>\n",
        "      <td>72</td>\n",
@@ -289,7 +289,7 @@
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <td>1</td>\n",
+       "      <th>1</th>\n",
        "      <td>1</td>\n",
        "      <td>85</td>\n",
        "      <td>66</td>\n",
@@ -301,7 +301,7 @@
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <td>2</td>\n",
+       "      <th>2</th>\n",
        "      <td>8</td>\n",
        "      <td>183</td>\n",
        "      <td>64</td>\n",
@@ -313,7 +313,7 @@
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <td>3</td>\n",
+       "      <th>3</th>\n",
        "      <td>1</td>\n",
        "      <td>89</td>\n",
        "      <td>66</td>\n",
@@ -325,7 +325,7 @@
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <td>4</td>\n",
+       "      <th>4</th>\n",
        "      <td>0</td>\n",
        "      <td>137</td>\n",
        "      <td>40</td>\n",
@@ -349,7 +349,7 @@
        "4  0  137  40  35  168  43.1  2.288  33  1"
       ]
      },
-     "execution_count": 5,
+     "execution_count": 56,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -360,7 +360,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 57,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -394,7 +394,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 58,
    "metadata": {},
    "outputs": [
     {
@@ -455,7 +455,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 59,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -483,7 +483,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": 60,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -511,7 +511,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 17,
+   "execution_count": 61,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -545,7 +545,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 18,
+   "execution_count": 62,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -571,7 +571,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 19,
+   "execution_count": 63,
    "metadata": {},
    "outputs": [
     {
@@ -580,314 +580,314 @@
      "text": [
       "Train on 768 samples\n",
       "Epoch 1/150\n",
-      "768/768 [==============================] - 0s 441us/sample - loss: 0.6796 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 279us/sample - loss: 70.6642 - accuracy: 0.3477\n",
       "Epoch 2/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.6693 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 65.5551 - accuracy: 0.3477\n",
       "Epoch 3/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6572 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 60.4134 - accuracy: 0.3477\n",
       "Epoch 4/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6538 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 55.3356 - accuracy: 0.3490\n",
       "Epoch 5/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6517 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 50.2359 - accuracy: 0.3490\n",
       "Epoch 6/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6509 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 45.2298 - accuracy: 0.3490\n",
       "Epoch 7/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6493 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 30us/sample - loss: 40.0723 - accuracy: 0.3490\n",
       "Epoch 8/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6486 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 35.0567 - accuracy: 0.3503\n",
       "Epoch 9/150\n",
-      "768/768 [==============================] - 0s 58us/sample - loss: 0.6482 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 29.9718 - accuracy: 0.3529\n",
       "Epoch 10/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6473 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 25.1771 - accuracy: 0.3581\n",
       "Epoch 11/150\n",
-      "768/768 [==============================] - 0s 57us/sample - loss: 0.6468 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 20.5387 - accuracy: 0.3724\n",
       "Epoch 12/150\n",
-      "768/768 [==============================] - 0s 59us/sample - loss: 0.6469 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 16.4774 - accuracy: 0.3841\n",
       "Epoch 13/150\n",
-      "768/768 [==============================] - 0s 58us/sample - loss: 0.6461 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 13.3479 - accuracy: 0.4062\n",
       "Epoch 14/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6472 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 11.4589 - accuracy: 0.4219\n",
       "Epoch 15/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6459 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 10.3382 - accuracy: 0.4362\n",
       "Epoch 16/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6457 - accuracy: 0.6576\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 9.6603 - accuracy: 0.4688\n",
       "Epoch 17/150\n",
-      "768/768 [==============================] - 0s 59us/sample - loss: 0.6455 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 9.1744 - accuracy: 0.4779\n",
       "Epoch 18/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6455 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 8.7853 - accuracy: 0.4870\n",
       "Epoch 19/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6453 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 8.4598 - accuracy: 0.4883\n",
       "Epoch 20/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6453 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 8.1677 - accuracy: 0.5000\n",
       "Epoch 21/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6452 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 38us/sample - loss: 7.9005 - accuracy: 0.5104\n",
       "Epoch 22/150\n",
-      "768/768 [==============================] - 0s 59us/sample - loss: 0.6452 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 7.6617 - accuracy: 0.5104\n",
       "Epoch 23/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6453 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 7.4511 - accuracy: 0.5182\n",
       "Epoch 24/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6450 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 7.2660 - accuracy: 0.5247\n",
       "Epoch 25/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6450 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 7.1004 - accuracy: 0.5273\n",
       "Epoch 26/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6450 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 6.9601 - accuracy: 0.5339\n",
       "Epoch 27/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6449 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 6.8383 - accuracy: 0.5378\n",
       "Epoch 28/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6448 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 30us/sample - loss: 6.7307 - accuracy: 0.5391\n",
       "Epoch 29/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6448 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 6.6388 - accuracy: 0.5443\n",
       "Epoch 30/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6447 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 6.5526 - accuracy: 0.5482\n",
       "Epoch 31/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6451 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 6.4806 - accuracy: 0.5456\n",
       "Epoch 32/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6446 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 6.4032 - accuracy: 0.5482\n",
       "Epoch 33/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6446 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 6.3407 - accuracy: 0.5456\n",
       "Epoch 34/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6445 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 6.2624 - accuracy: 0.5443\n",
       "Epoch 35/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6447 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 37us/sample - loss: 6.1991 - accuracy: 0.5456\n",
       "Epoch 36/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6446 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 6.1375 - accuracy: 0.5482\n",
       "Epoch 37/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6446 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 6.0752 - accuracy: 0.5469\n",
       "Epoch 38/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6447 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 6.0144 - accuracy: 0.5456\n",
       "Epoch 39/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6442 - accuracy: 0.6562\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 5.9539 - accuracy: 0.5443\n",
       "Epoch 40/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6445 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 5.8874 - accuracy: 0.5430\n",
       "Epoch 41/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6443 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 5.8313 - accuracy: 0.5430\n",
       "Epoch 42/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6448 - accuracy: 0.6497\n",
+      "768/768 [==============================] - 0s 30us/sample - loss: 5.7684 - accuracy: 0.5417\n",
       "Epoch 43/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6440 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 5.7054 - accuracy: 0.5430\n",
       "Epoch 44/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.6430 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 5.6422 - accuracy: 0.5443\n",
       "Epoch 45/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6432 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 5.5800 - accuracy: 0.5417\n",
       "Epoch 46/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6420 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 5.5185 - accuracy: 0.5404\n",
       "Epoch 47/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6407 - accuracy: 0.6471\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 5.4518 - accuracy: 0.5404\n",
       "Epoch 48/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6421 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 5.3900 - accuracy: 0.5404\n",
       "Epoch 49/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6390 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 5.3412 - accuracy: 0.5430\n",
       "Epoch 50/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6410 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 5.2642 - accuracy: 0.5443\n",
       "Epoch 51/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6394 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 5.2005 - accuracy: 0.5443\n",
       "Epoch 52/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6390 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 5.1477 - accuracy: 0.5508\n",
       "Epoch 53/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6369 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 5.0695 - accuracy: 0.5482\n",
       "Epoch 54/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6365 - accuracy: 0.6576\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 5.0020 - accuracy: 0.5508\n",
       "Epoch 55/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6361 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 30us/sample - loss: 4.9384 - accuracy: 0.5521\n",
       "Epoch 56/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6357 - accuracy: 0.6589\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 4.8692 - accuracy: 0.5508\n",
       "Epoch 57/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6354 - accuracy: 0.6562\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 4.8135 - accuracy: 0.5521\n",
       "Epoch 58/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6350 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 4.7390 - accuracy: 0.5534\n",
       "Epoch 59/150\n",
-      "768/768 [==============================] - 0s 59us/sample - loss: 0.6345 - accuracy: 0.6562\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 4.6696 - accuracy: 0.5521\n",
       "Epoch 60/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6366 - accuracy: 0.6497\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 4.6041 - accuracy: 0.5534\n",
       "Epoch 61/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6357 - accuracy: 0.6562\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 4.5387 - accuracy: 0.5534\n",
       "Epoch 62/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6338 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 4.4658 - accuracy: 0.5508\n",
       "Epoch 63/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6338 - accuracy: 0.6589\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 4.4000 - accuracy: 0.5521\n",
       "Epoch 64/150\n",
-      "768/768 [==============================] - 0s 66us/sample - loss: 0.6328 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 4.3384 - accuracy: 0.5521\n",
       "Epoch 65/150\n",
-      "768/768 [==============================] - 0s 66us/sample - loss: 0.6326 - accuracy: 0.6589\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 4.2659 - accuracy: 0.5534\n",
       "Epoch 66/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6320 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 4.1892 - accuracy: 0.5521\n",
       "Epoch 67/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6315 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 4.1298 - accuracy: 0.5508\n",
       "Epoch 68/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.6314 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 4.0604 - accuracy: 0.5521\n",
       "Epoch 69/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6306 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 3.9855 - accuracy: 0.5534\n",
       "Epoch 70/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6303 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 3.9184 - accuracy: 0.5534\n",
       "Epoch 71/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6299 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 3.8484 - accuracy: 0.5521\n",
       "Epoch 72/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6296 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 3.7780 - accuracy: 0.5495\n",
       "Epoch 73/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6299 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 3.7072 - accuracy: 0.5482\n",
       "Epoch 74/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6290 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 3.6376 - accuracy: 0.5560\n",
       "Epoch 75/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6291 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 3.5828 - accuracy: 0.5456\n",
       "Epoch 76/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6286 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 3.5002 - accuracy: 0.5547\n",
       "Epoch 77/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6283 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 3.4322 - accuracy: 0.5560\n",
       "Epoch 78/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.6275 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 3.3633 - accuracy: 0.5508\n",
       "Epoch 79/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6249 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 3.2902 - accuracy: 0.5560\n",
       "Epoch 80/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6207 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 3.2215 - accuracy: 0.5560\n",
       "Epoch 81/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6146 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 3.1581 - accuracy: 0.5560\n",
       "Epoch 82/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6101 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 3.0871 - accuracy: 0.5586\n",
       "Epoch 83/150\n",
-      "768/768 [==============================] - 0s 69us/sample - loss: 0.6076 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 3.0160 - accuracy: 0.5612\n",
       "Epoch 84/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6070 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 2.9470 - accuracy: 0.5625\n",
       "Epoch 85/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6037 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 2.8816 - accuracy: 0.5586\n",
       "Epoch 86/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6058 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 2.8311 - accuracy: 0.5612\n",
       "Epoch 87/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6026 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 2.7565 - accuracy: 0.5651\n",
       "Epoch 88/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6050 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 2.6908 - accuracy: 0.5573\n",
       "Epoch 89/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6014 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 2.6157 - accuracy: 0.5625\n",
       "Epoch 90/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6011 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 38us/sample - loss: 2.5525 - accuracy: 0.5534\n",
       "Epoch 91/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5990 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 2.4869 - accuracy: 0.5612\n",
       "Epoch 92/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6027 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 2.4297 - accuracy: 0.5586\n",
       "Epoch 93/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5987 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 2.3554 - accuracy: 0.5586\n",
       "Epoch 94/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6013 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 2.2971 - accuracy: 0.5651\n",
       "Epoch 95/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5984 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 2.2345 - accuracy: 0.5651\n",
       "Epoch 96/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5979 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 2.1710 - accuracy: 0.5586\n",
       "Epoch 97/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5977 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 30us/sample - loss: 2.1121 - accuracy: 0.5612\n",
       "Epoch 98/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5968 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 2.0531 - accuracy: 0.5625\n",
       "Epoch 99/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5957 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 1.9945 - accuracy: 0.5573\n",
       "Epoch 100/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5955 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 1.9306 - accuracy: 0.5677\n",
       "Epoch 101/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5963 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 1.8704 - accuracy: 0.5690\n",
       "Epoch 102/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5923 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 1.8248 - accuracy: 0.5638\n",
       "Epoch 103/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5957 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 38us/sample - loss: 1.7641 - accuracy: 0.5599\n",
       "Epoch 104/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5941 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 1.7118 - accuracy: 0.5703\n",
       "Epoch 105/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.5925 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 1.6548 - accuracy: 0.5664\n",
       "Epoch 106/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5923 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 1.6184 - accuracy: 0.5664\n",
       "Epoch 107/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5916 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 1.5515 - accuracy: 0.5938\n",
       "Epoch 108/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5925 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 1.4980 - accuracy: 0.5768\n",
       "Epoch 109/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5914 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 1.4515 - accuracy: 0.5872\n",
       "Epoch 110/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5904 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 1.4026 - accuracy: 0.5964\n",
       "Epoch 111/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5901 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 1.3611 - accuracy: 0.5911\n",
       "Epoch 112/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5949 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 1.3148 - accuracy: 0.6042\n",
       "Epoch 113/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5920 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 1.2704 - accuracy: 0.5964\n",
       "Epoch 114/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5904 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 1.2279 - accuracy: 0.6003\n",
       "Epoch 115/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5902 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 38us/sample - loss: 1.1915 - accuracy: 0.6003\n",
       "Epoch 116/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5893 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 1.1505 - accuracy: 0.6068\n",
       "Epoch 117/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5892 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 1.1160 - accuracy: 0.6185\n",
       "Epoch 118/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.5889 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 1.0831 - accuracy: 0.6107\n",
       "Epoch 119/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5877 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 1.0532 - accuracy: 0.6159\n",
       "Epoch 120/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.5878 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 38us/sample - loss: 1.0220 - accuracy: 0.6172\n",
       "Epoch 121/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5880 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 0.9996 - accuracy: 0.6029\n",
       "Epoch 122/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5869 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 0.9686 - accuracy: 0.6406\n",
       "Epoch 123/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.5872 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 0.9412 - accuracy: 0.6237\n",
       "Epoch 124/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5874 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 0.9099 - accuracy: 0.6289\n",
       "Epoch 125/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5902 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 0.8891 - accuracy: 0.6341\n",
       "Epoch 126/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5862 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 0.8675 - accuracy: 0.6445\n",
       "Epoch 127/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5864 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 0.8493 - accuracy: 0.6354\n",
       "Epoch 128/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5858 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 0.8319 - accuracy: 0.6328\n",
       "Epoch 129/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5851 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 0.8099 - accuracy: 0.6484\n",
       "Epoch 130/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5868 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 0.7950 - accuracy: 0.6458\n",
       "Epoch 131/150\n",
-      "768/768 [==============================] - 0s 70us/sample - loss: 0.5855 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 0.7855 - accuracy: 0.6602\n",
       "Epoch 132/150\n",
-      "768/768 [==============================] - 0s 66us/sample - loss: 0.5862 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 31us/sample - loss: 0.7725 - accuracy: 0.6576\n",
       "Epoch 133/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5842 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 0.7789 - accuracy: 0.6549\n",
       "Epoch 134/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5866 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 0.7581 - accuracy: 0.6628\n",
       "Epoch 135/150\n",
-      "768/768 [==============================] - 0s 59us/sample - loss: 0.5841 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.7363 - accuracy: 0.6680\n",
       "Epoch 136/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5836 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 0.7267 - accuracy: 0.6784\n",
       "Epoch 137/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5852 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 0.7125 - accuracy: 0.6693\n",
       "Epoch 138/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5845 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 0.7108 - accuracy: 0.6732\n",
       "Epoch 139/150\n",
-      "768/768 [==============================] - 0s 75us/sample - loss: 0.5840 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 0.6994 - accuracy: 0.6953\n",
       "Epoch 140/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.5836 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 0.6897 - accuracy: 0.6836\n",
       "Epoch 141/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5843 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 0.6827 - accuracy: 0.6862\n",
       "Epoch 142/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5828 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 0.6805 - accuracy: 0.6836\n",
       "Epoch 143/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5829 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 0.6698 - accuracy: 0.6953\n",
       "Epoch 144/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5829 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 0.6648 - accuracy: 0.7044\n",
       "Epoch 145/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5818 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 33us/sample - loss: 0.6597 - accuracy: 0.7018\n",
       "Epoch 146/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5832 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.6519 - accuracy: 0.6979\n",
       "Epoch 147/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5813 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 35us/sample - loss: 0.6467 - accuracy: 0.7057\n",
       "Epoch 148/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.5832 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 0.6411 - accuracy: 0.6992\n",
       "Epoch 149/150\n",
-      "768/768 [==============================] - 0s 66us/sample - loss: 0.5808 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 0.6400 - accuracy: 0.7018\n",
       "Epoch 150/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5806 - accuracy: 0.6510\n"
+      "768/768 [==============================] - 0s 35us/sample - loss: 0.6367 - accuracy: 0.7122\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7fb4cc230668>"
+       "<tensorflow.python.keras.callbacks.History at 0x186cc8df2c8>"
       ]
      },
-     "execution_count": 19,
+     "execution_count": 63,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -898,7 +898,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 64,
    "metadata": {},
    "outputs": [
     {
@@ -907,7 +907,7 @@
        "(768,)"
       ]
      },
-     "execution_count": 13,
+     "execution_count": 64,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -925,7 +925,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 14,
+   "execution_count": 65,
    "metadata": {},
    "outputs": [
     {
@@ -936,7 +936,7 @@
        "       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.])"
       ]
      },
-     "execution_count": 14,
+     "execution_count": 65,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -947,7 +947,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 15,
+   "execution_count": 66,
    "metadata": {},
    "outputs": [
     {
@@ -956,7 +956,7 @@
        "0.3489583333333333"
       ]
      },
-     "execution_count": 15,
+     "execution_count": 66,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -968,15 +968,15 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 20,
+   "execution_count": 67,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "768/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 142us/sample - loss: 0.6205 - accuracy: 0.6510\n",
-      "accuracy: 65.10416865348816\n"
+      "768/768 [==============================] - 0s 92us/sample - loss: 0.6278 - accuracy: 0.7044\n",
+      "accuracy: 70.44270634651184\n"
      ]
     }
    ],
@@ -1036,7 +1036,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 22,
+   "execution_count": 68,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -1053,9 +1053,9 @@
       "=================================================================\n",
       "Dense1 (Dense)               (None, 4)                 36        \n",
       "_________________________________________________________________\n",
-      "dense_6 (Dense)              (None, 3)                 15        \n",
+      "dense_56 (Dense)             (None, 3)                 15        \n",
       "_________________________________________________________________\n",
-      "dense_7 (Dense)              (None, 1)                 4         \n",
+      "dense_57 (Dense)             (None, 1)                 4         \n",
       "=================================================================\n",
       "Total params: 55\n",
       "Trainable params: 55\n",
@@ -1082,16 +1082,16 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 24,
+   "execution_count": 69,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7fb4306ce2e8>"
+       "<tensorflow.python.keras.callbacks.History at 0x186ccaaf508>"
       ]
      },
-     "execution_count": 24,
+     "execution_count": 69,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -1102,15 +1102,15 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 28,
+   "execution_count": 70,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "768/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 68us/sample - loss: 0.5155 - accuracy: 0.7578\n",
-      "accuracy: 75.78125\n"
+      "768/768 [==============================] - 0s 95us/sample - loss: 0.6468 - accuracy: 0.6510\n",
+      "accuracy: 65.10416865348816\n"
      ]
     }
    ],
@@ -1128,7 +1128,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 29,
+   "execution_count": 71,
    "metadata": {},
    "outputs": [
     {
@@ -1141,11 +1141,11 @@
       "=================================================================\n",
       "Dense1 (Dense)               (None, 4)                 36        \n",
       "_________________________________________________________________\n",
-      "dense_8 (Dense)              (None, 3)                 15        \n",
+      "dense_58 (Dense)             (None, 3)                 15        \n",
       "_________________________________________________________________\n",
-      "dense_9 (Dense)              (None, 3)                 12        \n",
+      "dense_59 (Dense)             (None, 3)                 12        \n",
       "_________________________________________________________________\n",
-      "dense_10 (Dense)             (None, 1)                 4         \n",
+      "dense_60 (Dense)             (None, 1)                 4         \n",
       "=================================================================\n",
       "Total params: 67\n",
       "Trainable params: 67\n",
@@ -1173,16 +1173,16 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 30,
+   "execution_count": 72,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7fb430342ac8>"
+       "<tensorflow.python.keras.callbacks.History at 0x186cdc04c88>"
       ]
      },
-     "execution_count": 30,
+     "execution_count": 72,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -1193,23 +1193,23 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 31,
+   "execution_count": 73,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "768/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 169us/sample - loss: 0.6716 - accuracy: 0.6510\n"
+      "768/768 [==============================] - 0s 99us/sample - loss: 0.6468 - accuracy: 0.6510\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "[0.639621468881766, 0.6510417]"
+       "[0.6467994749546051, 0.6510417]"
       ]
      },
-     "execution_count": 31,
+     "execution_count": 73,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -1228,7 +1228,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 32,
+   "execution_count": 74,
    "metadata": {},
    "outputs": [
     {
@@ -1241,11 +1241,11 @@
       "=================================================================\n",
       "Dense1 (Dense)               (None, 8)                 72        \n",
       "_________________________________________________________________\n",
-      "dense_11 (Dense)             (None, 8)                 72        \n",
+      "dense_61 (Dense)             (None, 8)                 72        \n",
       "_________________________________________________________________\n",
-      "dense_12 (Dense)             (None, 8)                 72        \n",
+      "dense_62 (Dense)             (None, 8)                 72        \n",
       "_________________________________________________________________\n",
-      "dense_13 (Dense)             (None, 1)                 9         \n",
+      "dense_63 (Dense)             (None, 1)                 9         \n",
       "=================================================================\n",
       "Total params: 225\n",
       "Trainable params: 225\n",
@@ -1273,557 +1273,1223 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 37,
+   "execution_count": 75,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "<tensorflow.python.keras.callbacks.History at 0x186cddd1c88>"
+      ]
+     },
+     "execution_count": 75,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "model_improved.fit(X,y, epochs=250, validation_split=.10, verbose=False)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 76,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "768/768 [==============================] - 0s 98us/sample - loss: 0.5352 - accuracy: 0.7057\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "[0.5352091950674852, 0.7057292]"
+      ]
+     },
+     "execution_count": 76,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "model_improved.evaluate(X,y)\n",
+    "#print(f\"{model_improved.metrics_names[1]}: {scores[1]*100}\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Experimenting\n",
+    "\n",
+    "10 minutes researching keras API docs or general NN architectures.\n",
+    "1. optimizers\n",
+    "2. loss metrics\n",
+    "3. activation functions\n",
+    "4. parameters\n",
+    "5. number of layers/neurons"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 77,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Model: \"Nadamclipnorm\"\n",
+      "_________________________________________________________________\n",
+      "Layer (type)                 Output Shape              Param #   \n",
+      "=================================================================\n",
+      "dense_64 (Dense)             (None, 8)                 72        \n",
+      "_________________________________________________________________\n",
+      "dense_65 (Dense)             (None, 8)                 72        \n",
+      "_________________________________________________________________\n",
+      "dense_66 (Dense)             (None, 3)                 27        \n",
+      "_________________________________________________________________\n",
+      "dense_67 (Dense)             (None, 3)                 12        \n",
+      "_________________________________________________________________\n",
+      "dense_68 (Dense)             (None, 1)                 4         \n",
+      "=================================================================\n",
+      "Total params: 187\n",
+      "Trainable params: 187\n",
+      "Non-trainable params: 0\n",
+      "_________________________________________________________________\n"
+     ]
+    }
+   ],
+   "source": [
+    "from tensorflow.keras.optimizers import Nadam\n",
+    "\n",
+    "nadam = Nadam(clipnorm=.7)\n",
+    "\n",
+    "model_improved = Sequential(name=\"Nadamclipnorm\")\n",
+    "\n",
+    "model_improved.add(Dense(8, input_dim=8, activation='relu'))\n",
+    "model_improved.add(Dense(8, activation='relu'))\n",
+    "model_improved.add(Dense(3, activation='relu'))\n",
+    "model_improved.add(Dense(3, activation='relu'))\n",
+    "model_improved.add(Dense(1, activation='sigmoid'))\n",
+    "\n",
+    "model_improved.compile(loss='binary_crossentropy', optimizer=nadam,\n",
+    "              metrics=['accuracy'])\n",
+    "\n",
+    "# Let's inspect our new architecture\n",
+    "model_improved.summary()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 78,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Train on 691 samples, validate on 77 samples\n",
+      "Train on 768 samples\n",
       "Epoch 1/250\n",
-      "691/691 [==============================] - 0s 132us/sample - loss: 0.4450 - accuracy: 0.7844 - val_loss: 0.5430 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 1s 716us/sample - loss: 2.3667 - accuracy: 0.3503\n",
       "Epoch 2/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4350 - accuracy: 0.7931 - val_loss: 0.5368 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 1.4784 - accuracy: 0.3516\n",
       "Epoch 3/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4341 - accuracy: 0.8003 - val_loss: 0.5612 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 1.0410 - accuracy: 0.3529\n",
       "Epoch 4/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4432 - accuracy: 0.7786 - val_loss: 0.5810 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.8530 - accuracy: 0.3594\n",
       "Epoch 5/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4378 - accuracy: 0.7858 - val_loss: 0.5426 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.7693 - accuracy: 0.4688\n",
       "Epoch 6/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4341 - accuracy: 0.7815 - val_loss: 0.5429 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.7292 - accuracy: 0.5352\n",
       "Epoch 7/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4304 - accuracy: 0.8075 - val_loss: 0.5942 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.7111 - accuracy: 0.6003\n",
       "Epoch 8/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4388 - accuracy: 0.7829 - val_loss: 0.5631 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.7000 - accuracy: 0.5990\n",
       "Epoch 9/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4377 - accuracy: 0.7728 - val_loss: 0.5333 - val_accuracy: 0.8052\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6914 - accuracy: 0.6068\n",
       "Epoch 10/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4476 - accuracy: 0.7815 - val_loss: 0.5622 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6844 - accuracy: 0.6289\n",
       "Epoch 11/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4351 - accuracy: 0.7945 - val_loss: 0.6104 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6796 - accuracy: 0.6341\n",
       "Epoch 12/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4468 - accuracy: 0.7902 - val_loss: 0.5943 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6757 - accuracy: 0.6445\n",
       "Epoch 13/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4304 - accuracy: 0.7873 - val_loss: 0.5597 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6729 - accuracy: 0.6471\n",
       "Epoch 14/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4454 - accuracy: 0.7800 - val_loss: 0.5892 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6699 - accuracy: 0.6549\n",
       "Epoch 15/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4544 - accuracy: 0.7670 - val_loss: 0.5805 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6675 - accuracy: 0.6576\n",
       "Epoch 16/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4406 - accuracy: 0.7829 - val_loss: 0.5796 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6636 - accuracy: 0.6628\n",
       "Epoch 17/250\n",
-      "691/691 [==============================] - 0s 116us/sample - loss: 0.4325 - accuracy: 0.7887 - val_loss: 0.5502 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.6562 - accuracy: 0.6719\n",
       "Epoch 18/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4376 - accuracy: 0.7800 - val_loss: 0.5716 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6479 - accuracy: 0.6667\n",
       "Epoch 19/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4293 - accuracy: 0.7844 - val_loss: 0.5916 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6406 - accuracy: 0.6745\n",
       "Epoch 20/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4372 - accuracy: 0.7902 - val_loss: 0.5735 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6366 - accuracy: 0.6758\n",
       "Epoch 21/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4367 - accuracy: 0.7844 - val_loss: 0.5870 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6340 - accuracy: 0.6784\n",
       "Epoch 22/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4360 - accuracy: 0.7916 - val_loss: 0.5698 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.6307 - accuracy: 0.6797\n",
       "Epoch 23/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4375 - accuracy: 0.7873 - val_loss: 0.5782 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 38us/sample - loss: 0.6292 - accuracy: 0.6784\n",
       "Epoch 24/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4428 - accuracy: 0.7887 - val_loss: 0.5882 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.6257 - accuracy: 0.6836\n",
       "Epoch 25/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4320 - accuracy: 0.7916 - val_loss: 0.5487 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6243 - accuracy: 0.6810\n",
       "Epoch 26/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4361 - accuracy: 0.7959 - val_loss: 0.6589 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6223 - accuracy: 0.6784\n",
       "Epoch 27/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4681 - accuracy: 0.7902 - val_loss: 0.5854 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6200 - accuracy: 0.6823\n",
       "Epoch 28/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4400 - accuracy: 0.7800 - val_loss: 0.5625 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6184 - accuracy: 0.6862\n",
       "Epoch 29/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4297 - accuracy: 0.7858 - val_loss: 0.5486 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6174 - accuracy: 0.6875\n",
       "Epoch 30/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4304 - accuracy: 0.7931 - val_loss: 0.5471 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6155 - accuracy: 0.6875\n",
       "Epoch 31/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4308 - accuracy: 0.7916 - val_loss: 0.5568 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.6148 - accuracy: 0.6797\n",
       "Epoch 32/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4303 - accuracy: 0.8017 - val_loss: 0.5375 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6124 - accuracy: 0.6914\n",
       "Epoch 33/250\n",
-      "691/691 [==============================] - 0s 102us/sample - loss: 0.4359 - accuracy: 0.7800 - val_loss: 0.5793 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.6123 - accuracy: 0.6823\n",
       "Epoch 34/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4341 - accuracy: 0.7873 - val_loss: 0.5466 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6122 - accuracy: 0.6927\n",
       "Epoch 35/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4293 - accuracy: 0.7858 - val_loss: 0.5446 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6121 - accuracy: 0.6875\n",
       "Epoch 36/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4400 - accuracy: 0.7887 - val_loss: 0.5486 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6098 - accuracy: 0.6979\n",
       "Epoch 37/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4373 - accuracy: 0.7815 - val_loss: 0.5552 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6093 - accuracy: 0.6914\n",
       "Epoch 38/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4380 - accuracy: 0.7916 - val_loss: 0.5937 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6078 - accuracy: 0.6940\n",
       "Epoch 39/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4321 - accuracy: 0.7902 - val_loss: 0.5762 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6076 - accuracy: 0.6927\n",
       "Epoch 40/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4344 - accuracy: 0.7844 - val_loss: 0.5833 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.6076 - accuracy: 0.6940\n",
       "Epoch 41/250\n",
-      "691/691 [==============================] - 0s 122us/sample - loss: 0.4319 - accuracy: 0.7945 - val_loss: 0.5713 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.6068 - accuracy: 0.6927\n",
       "Epoch 42/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4466 - accuracy: 0.7713 - val_loss: 0.5554 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6064 - accuracy: 0.6966\n",
       "Epoch 43/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4512 - accuracy: 0.7699 - val_loss: 0.5515 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.6069 - accuracy: 0.7005\n",
       "Epoch 44/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4295 - accuracy: 0.7916 - val_loss: 0.5527 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6045 - accuracy: 0.6966\n",
       "Epoch 45/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4415 - accuracy: 0.7858 - val_loss: 0.5779 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.6042 - accuracy: 0.6979\n",
       "Epoch 46/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4273 - accuracy: 0.7988 - val_loss: 0.5882 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.6037 - accuracy: 0.6966\n",
       "Epoch 47/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4365 - accuracy: 0.7916 - val_loss: 0.5730 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6035 - accuracy: 0.7005\n",
       "Epoch 48/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4475 - accuracy: 0.7771 - val_loss: 0.6129 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6009 - accuracy: 0.7070\n",
       "Epoch 49/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4330 - accuracy: 0.7844 - val_loss: 0.5698 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5992 - accuracy: 0.7005\n",
       "Epoch 50/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4265 - accuracy: 0.7786 - val_loss: 0.6032 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5976 - accuracy: 0.7083\n",
       "Epoch 51/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4355 - accuracy: 0.7873 - val_loss: 0.6040 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5998 - accuracy: 0.6992\n",
       "Epoch 52/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4307 - accuracy: 0.7829 - val_loss: 0.5508 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5976 - accuracy: 0.7031\n",
       "Epoch 53/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4514 - accuracy: 0.7786 - val_loss: 0.5587 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5972 - accuracy: 0.7018\n",
       "Epoch 54/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4318 - accuracy: 0.7902 - val_loss: 0.5885 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5958 - accuracy: 0.6992\n",
       "Epoch 55/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4512 - accuracy: 0.7815 - val_loss: 0.5813 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5966 - accuracy: 0.7031\n",
       "Epoch 56/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4363 - accuracy: 0.7844 - val_loss: 0.5499 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5967 - accuracy: 0.7018\n",
       "Epoch 57/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4327 - accuracy: 0.7902 - val_loss: 0.5899 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.5950 - accuracy: 0.7044\n",
       "Epoch 58/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4376 - accuracy: 0.7945 - val_loss: 0.5644 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5952 - accuracy: 0.7044\n",
       "Epoch 59/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4308 - accuracy: 0.7902 - val_loss: 0.5926 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5942 - accuracy: 0.7018\n",
       "Epoch 60/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4364 - accuracy: 0.7844 - val_loss: 0.5701 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5935 - accuracy: 0.7057\n",
       "Epoch 61/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4334 - accuracy: 0.7945 - val_loss: 0.5964 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5918 - accuracy: 0.7083\n",
       "Epoch 62/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4349 - accuracy: 0.7829 - val_loss: 0.5404 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5918 - accuracy: 0.7031\n",
       "Epoch 63/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4290 - accuracy: 0.7959 - val_loss: 0.5447 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5901 - accuracy: 0.7083\n",
       "Epoch 64/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4327 - accuracy: 0.7959 - val_loss: 0.5909 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.5811 - accuracy: 0.7057\n",
       "Epoch 65/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4352 - accuracy: 0.7916 - val_loss: 0.5384 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5729 - accuracy: 0.7083\n",
       "Epoch 66/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4386 - accuracy: 0.7800 - val_loss: 0.5673 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5691 - accuracy: 0.7096\n",
       "Epoch 67/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4465 - accuracy: 0.7800 - val_loss: 0.5630 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5644 - accuracy: 0.7096\n",
       "Epoch 68/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4288 - accuracy: 0.7887 - val_loss: 0.5608 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5637 - accuracy: 0.7227\n",
       "Epoch 69/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4379 - accuracy: 0.7771 - val_loss: 0.5875 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5566 - accuracy: 0.7305\n",
       "Epoch 70/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4208 - accuracy: 0.7931 - val_loss: 0.5458 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5562 - accuracy: 0.7201\n",
       "Epoch 71/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4375 - accuracy: 0.7858 - val_loss: 0.5575 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5536 - accuracy: 0.7188\n",
       "Epoch 72/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4269 - accuracy: 0.7902 - val_loss: 0.5839 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5530 - accuracy: 0.7253\n",
       "Epoch 73/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4352 - accuracy: 0.7887 - val_loss: 0.5685 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5492 - accuracy: 0.7201\n",
       "Epoch 74/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4353 - accuracy: 0.7800 - val_loss: 0.5833 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5457 - accuracy: 0.7305\n",
       "Epoch 75/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4339 - accuracy: 0.7873 - val_loss: 0.6230 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.5457 - accuracy: 0.7318\n",
       "Epoch 76/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4517 - accuracy: 0.7844 - val_loss: 0.5550 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5414 - accuracy: 0.7344\n",
       "Epoch 77/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4286 - accuracy: 0.7916 - val_loss: 0.5464 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5393 - accuracy: 0.7552\n",
       "Epoch 78/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4311 - accuracy: 0.7829 - val_loss: 0.5428 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5400 - accuracy: 0.7448\n",
       "Epoch 79/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4317 - accuracy: 0.7873 - val_loss: 0.5425 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5377 - accuracy: 0.7305\n",
       "Epoch 80/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4291 - accuracy: 0.7844 - val_loss: 0.5598 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5367 - accuracy: 0.7474\n",
       "Epoch 81/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4272 - accuracy: 0.7931 - val_loss: 0.5490 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5392 - accuracy: 0.7292\n",
       "Epoch 82/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4406 - accuracy: 0.7786 - val_loss: 0.5553 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5318 - accuracy: 0.7318\n",
       "Epoch 83/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4359 - accuracy: 0.7902 - val_loss: 0.5498 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5332 - accuracy: 0.7461\n",
       "Epoch 84/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4296 - accuracy: 0.7887 - val_loss: 0.5613 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.5362 - accuracy: 0.7409\n",
       "Epoch 85/250\n",
-      "691/691 [==============================] - 0s 102us/sample - loss: 0.4312 - accuracy: 0.7829 - val_loss: 0.5609 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5284 - accuracy: 0.7474\n",
       "Epoch 86/250\n",
-      "691/691 [==============================] - 0s 103us/sample - loss: 0.4378 - accuracy: 0.7829 - val_loss: 0.5826 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5284 - accuracy: 0.7513\n",
       "Epoch 87/250\n",
-      "691/691 [==============================] - 0s 102us/sample - loss: 0.4405 - accuracy: 0.7815 - val_loss: 0.5750 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5278 - accuracy: 0.7409\n",
       "Epoch 88/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4417 - accuracy: 0.7916 - val_loss: 0.5690 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5289 - accuracy: 0.7422\n",
       "Epoch 89/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4369 - accuracy: 0.7815 - val_loss: 0.5858 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.5258 - accuracy: 0.7357\n",
       "Epoch 90/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4374 - accuracy: 0.7800 - val_loss: 0.5682 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5266 - accuracy: 0.7448\n",
       "Epoch 91/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4356 - accuracy: 0.7844 - val_loss: 0.5611 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5244 - accuracy: 0.7448\n",
       "Epoch 92/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4474 - accuracy: 0.7815 - val_loss: 0.6017 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5264 - accuracy: 0.7500\n",
       "Epoch 93/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4294 - accuracy: 0.7945 - val_loss: 0.5852 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5213 - accuracy: 0.7474\n",
       "Epoch 94/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4310 - accuracy: 0.7844 - val_loss: 0.5812 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5234 - accuracy: 0.7474\n",
       "Epoch 95/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4336 - accuracy: 0.7902 - val_loss: 0.5876 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.5212 - accuracy: 0.7461\n",
       "Epoch 96/250\n",
-      "691/691 [==============================] - 0s 100us/sample - loss: 0.4403 - accuracy: 0.7713 - val_loss: 0.6064 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5186 - accuracy: 0.7539\n",
       "Epoch 97/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4504 - accuracy: 0.7800 - val_loss: 0.5883 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5205 - accuracy: 0.7487\n",
       "Epoch 98/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4353 - accuracy: 0.7988 - val_loss: 0.5885 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5191 - accuracy: 0.7435\n",
       "Epoch 99/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4357 - accuracy: 0.7757 - val_loss: 0.5428 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5148 - accuracy: 0.7435\n",
       "Epoch 100/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4440 - accuracy: 0.7728 - val_loss: 0.5738 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5168 - accuracy: 0.7448\n",
       "Epoch 101/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4327 - accuracy: 0.7829 - val_loss: 0.5985 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5124 - accuracy: 0.7526\n",
       "Epoch 102/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4354 - accuracy: 0.7887 - val_loss: 0.5707 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5133 - accuracy: 0.7539\n",
       "Epoch 103/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4529 - accuracy: 0.7829 - val_loss: 0.6645 - val_accuracy: 0.6364\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5153 - accuracy: 0.7500\n",
       "Epoch 104/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4368 - accuracy: 0.7829 - val_loss: 0.5762 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5106 - accuracy: 0.7591\n",
       "Epoch 105/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4291 - accuracy: 0.7887 - val_loss: 0.5867 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5112 - accuracy: 0.7539\n",
       "Epoch 106/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4273 - accuracy: 0.7959 - val_loss: 0.6049 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5123 - accuracy: 0.7578\n",
       "Epoch 107/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4344 - accuracy: 0.7945 - val_loss: 0.5747 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.5090 - accuracy: 0.7526\n",
       "Epoch 108/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4319 - accuracy: 0.7959 - val_loss: 0.6106 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5064 - accuracy: 0.7526\n",
       "Epoch 109/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4308 - accuracy: 0.7887 - val_loss: 0.5425 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5070 - accuracy: 0.7565\n",
       "Epoch 110/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4328 - accuracy: 0.7945 - val_loss: 0.5476 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.5067 - accuracy: 0.7669\n",
       "Epoch 111/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4234 - accuracy: 0.7959 - val_loss: 0.7005 - val_accuracy: 0.6234\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5064 - accuracy: 0.7604\n",
       "Epoch 112/250\n",
-      "691/691 [==============================] - 0s 116us/sample - loss: 0.4554 - accuracy: 0.7728 - val_loss: 0.6428 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5042 - accuracy: 0.7617\n",
       "Epoch 113/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4315 - accuracy: 0.7945 - val_loss: 0.5558 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5023 - accuracy: 0.7682\n",
       "Epoch 114/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4346 - accuracy: 0.7916 - val_loss: 0.5521 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5026 - accuracy: 0.7565\n",
       "Epoch 115/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4328 - accuracy: 0.8003 - val_loss: 0.5748 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.5024 - accuracy: 0.7630\n",
       "Epoch 116/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4331 - accuracy: 0.7988 - val_loss: 0.5338 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4994 - accuracy: 0.7578\n",
       "Epoch 117/250\n",
-      "691/691 [==============================] - 0s 116us/sample - loss: 0.4311 - accuracy: 0.7959 - val_loss: 0.6022 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 38us/sample - loss: 0.4984 - accuracy: 0.7669\n",
       "Epoch 118/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4380 - accuracy: 0.7771 - val_loss: 0.5797 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4966 - accuracy: 0.7643\n",
       "Epoch 119/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4304 - accuracy: 0.7858 - val_loss: 0.5868 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4981 - accuracy: 0.7643\n",
       "Epoch 120/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4309 - accuracy: 0.7786 - val_loss: 0.5755 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4969 - accuracy: 0.7669\n",
       "Epoch 121/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4284 - accuracy: 0.7887 - val_loss: 0.5662 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4972 - accuracy: 0.7565\n",
       "Epoch 122/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4307 - accuracy: 0.7902 - val_loss: 0.5881 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4951 - accuracy: 0.7682\n",
       "Epoch 123/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4381 - accuracy: 0.7873 - val_loss: 0.6287 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4931 - accuracy: 0.7656\n",
       "Epoch 124/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4290 - accuracy: 0.7829 - val_loss: 0.5570 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.4946 - accuracy: 0.7734\n",
       "Epoch 125/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4253 - accuracy: 0.7873 - val_loss: 0.5644 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 38us/sample - loss: 0.4943 - accuracy: 0.7604\n",
       "Epoch 126/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4249 - accuracy: 0.7931 - val_loss: 0.5739 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 0.4943 - accuracy: 0.7578\n",
       "Epoch 127/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4247 - accuracy: 0.7959 - val_loss: 0.5544 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4962 - accuracy: 0.7604\n",
       "Epoch 128/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4322 - accuracy: 0.7844 - val_loss: 0.5472 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4912 - accuracy: 0.7682\n",
       "Epoch 129/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4285 - accuracy: 0.7916 - val_loss: 0.5964 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4916 - accuracy: 0.7669\n",
       "Epoch 130/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4282 - accuracy: 0.7959 - val_loss: 0.5719 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4917 - accuracy: 0.7630\n",
       "Epoch 131/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4262 - accuracy: 0.7873 - val_loss: 0.5588 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4905 - accuracy: 0.7734\n",
       "Epoch 132/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4320 - accuracy: 0.7959 - val_loss: 0.5484 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 65us/sample - loss: 0.4858 - accuracy: 0.7695\n",
       "Epoch 133/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4397 - accuracy: 0.7844 - val_loss: 0.5603 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4890 - accuracy: 0.7604\n",
       "Epoch 134/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4287 - accuracy: 0.7858 - val_loss: 0.5460 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4871 - accuracy: 0.7721\n",
       "Epoch 135/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4373 - accuracy: 0.7916 - val_loss: 0.5439 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4871 - accuracy: 0.7682\n",
       "Epoch 136/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4378 - accuracy: 0.7902 - val_loss: 0.5814 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4875 - accuracy: 0.7643\n",
       "Epoch 137/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4412 - accuracy: 0.7858 - val_loss: 0.5689 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4824 - accuracy: 0.7669\n",
       "Epoch 138/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4304 - accuracy: 0.7916 - val_loss: 0.5558 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.4867 - accuracy: 0.7656\n",
       "Epoch 139/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4278 - accuracy: 0.7974 - val_loss: 0.5501 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4824 - accuracy: 0.7773\n",
       "Epoch 140/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4295 - accuracy: 0.7844 - val_loss: 0.5862 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4822 - accuracy: 0.7682\n",
       "Epoch 141/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4348 - accuracy: 0.8003 - val_loss: 0.6358 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4812 - accuracy: 0.7682\n",
       "Epoch 142/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4422 - accuracy: 0.7800 - val_loss: 0.5796 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.4816 - accuracy: 0.7786\n",
       "Epoch 143/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4374 - accuracy: 0.7988 - val_loss: 0.5926 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4818 - accuracy: 0.7760\n",
       "Epoch 144/250\n",
-      "691/691 [==============================] - 0s 115us/sample - loss: 0.4365 - accuracy: 0.7844 - val_loss: 0.5565 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4793 - accuracy: 0.7799\n",
       "Epoch 145/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4276 - accuracy: 0.7959 - val_loss: 0.5469 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4795 - accuracy: 0.7826\n",
       "Epoch 146/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4273 - accuracy: 0.7945 - val_loss: 0.5813 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4810 - accuracy: 0.7747\n",
       "Epoch 147/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4271 - accuracy: 0.7844 - val_loss: 0.5794 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4783 - accuracy: 0.7708\n",
       "Epoch 148/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4253 - accuracy: 0.7974 - val_loss: 0.5377 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.4771 - accuracy: 0.7747\n",
       "Epoch 149/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4295 - accuracy: 0.7959 - val_loss: 0.5592 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4763 - accuracy: 0.7826\n",
       "Epoch 150/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4446 - accuracy: 0.7873 - val_loss: 0.5472 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4746 - accuracy: 0.7799\n",
       "Epoch 151/250\n",
-      "691/691 [==============================] - 0s 103us/sample - loss: 0.4275 - accuracy: 0.7959 - val_loss: 0.5726 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4745 - accuracy: 0.7865\n",
       "Epoch 152/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4350 - accuracy: 0.7887 - val_loss: 0.5549 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4743 - accuracy: 0.7799\n",
       "Epoch 153/250\n",
-      "691/691 [==============================] - 0s 103us/sample - loss: 0.4352 - accuracy: 0.7887 - val_loss: 0.5623 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 38us/sample - loss: 0.4740 - accuracy: 0.7773\n",
       "Epoch 154/250\n",
-      "691/691 [==============================] - 0s 103us/sample - loss: 0.4381 - accuracy: 0.7873 - val_loss: 0.6157 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4765 - accuracy: 0.7812\n",
       "Epoch 155/250\n",
-      "691/691 [==============================] - 0s 100us/sample - loss: 0.4451 - accuracy: 0.7786 - val_loss: 0.5586 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4744 - accuracy: 0.7786\n",
       "Epoch 156/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4294 - accuracy: 0.7902 - val_loss: 0.5669 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.4738 - accuracy: 0.7865\n",
       "Epoch 157/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4446 - accuracy: 0.7728 - val_loss: 0.5718 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4712 - accuracy: 0.7904\n",
       "Epoch 158/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4357 - accuracy: 0.7945 - val_loss: 0.5919 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4715 - accuracy: 0.7878\n",
       "Epoch 159/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4244 - accuracy: 0.7959 - val_loss: 0.6961 - val_accuracy: 0.5974\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4733 - accuracy: 0.7826\n",
       "Epoch 160/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4460 - accuracy: 0.7815 - val_loss: 0.6046 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4729 - accuracy: 0.7865\n",
       "Epoch 161/250\n",
-      "691/691 [==============================] - 0s 119us/sample - loss: 0.4392 - accuracy: 0.7800 - val_loss: 0.6032 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4700 - accuracy: 0.7917\n",
       "Epoch 162/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4257 - accuracy: 0.7844 - val_loss: 0.6199 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4681 - accuracy: 0.7839\n",
       "Epoch 163/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4304 - accuracy: 0.7916 - val_loss: 0.6495 - val_accuracy: 0.6364\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4694 - accuracy: 0.7812\n",
       "Epoch 164/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4354 - accuracy: 0.7988 - val_loss: 0.5772 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4692 - accuracy: 0.7852\n",
       "Epoch 165/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4322 - accuracy: 0.7757 - val_loss: 0.5907 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4681 - accuracy: 0.7786\n",
       "Epoch 166/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4383 - accuracy: 0.7844 - val_loss: 0.5795 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4648 - accuracy: 0.7878\n",
       "Epoch 167/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4323 - accuracy: 0.7931 - val_loss: 0.6260 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.4673 - accuracy: 0.7760\n",
       "Epoch 168/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4343 - accuracy: 0.7945 - val_loss: 0.5615 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4650 - accuracy: 0.7878\n",
       "Epoch 169/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4289 - accuracy: 0.7974 - val_loss: 0.5703 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.4646 - accuracy: 0.7891\n",
       "Epoch 170/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4280 - accuracy: 0.7902 - val_loss: 0.5718 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4654 - accuracy: 0.7904\n",
       "Epoch 171/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4257 - accuracy: 0.7873 - val_loss: 0.5567 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.4677 - accuracy: 0.7839\n",
       "Epoch 172/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4307 - accuracy: 0.7945 - val_loss: 0.5513 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4656 - accuracy: 0.7878\n",
       "Epoch 173/250\n",
-      "691/691 [==============================] - 0s 125us/sample - loss: 0.4239 - accuracy: 0.8017 - val_loss: 0.5830 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4615 - accuracy: 0.7865\n",
       "Epoch 174/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4255 - accuracy: 0.7974 - val_loss: 0.5419 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4666 - accuracy: 0.7956\n",
       "Epoch 175/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4613 - accuracy: 0.7713 - val_loss: 0.5484 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4628 - accuracy: 0.7917\n",
       "Epoch 176/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4456 - accuracy: 0.7916 - val_loss: 0.5627 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4613 - accuracy: 0.7982\n",
       "Epoch 177/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4381 - accuracy: 0.7916 - val_loss: 0.5449 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4594 - accuracy: 0.7760\n",
       "Epoch 178/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4512 - accuracy: 0.7771 - val_loss: 0.5280 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4597 - accuracy: 0.7917\n",
       "Epoch 179/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4309 - accuracy: 0.7988 - val_loss: 0.5866 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.4593 - accuracy: 0.7930\n",
       "Epoch 180/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4230 - accuracy: 0.7873 - val_loss: 0.5618 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4589 - accuracy: 0.7904\n",
       "Epoch 181/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4261 - accuracy: 0.7931 - val_loss: 0.5515 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.4600 - accuracy: 0.7826\n",
       "Epoch 182/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4267 - accuracy: 0.7974 - val_loss: 0.5404 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.4604 - accuracy: 0.7891\n",
       "Epoch 183/250\n",
-      "691/691 [==============================] - 0s 115us/sample - loss: 0.4336 - accuracy: 0.7974 - val_loss: 0.5765 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.4563 - accuracy: 0.7969\n",
       "Epoch 184/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4317 - accuracy: 0.7858 - val_loss: 0.5615 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.4570 - accuracy: 0.7878\n",
       "Epoch 185/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4294 - accuracy: 0.7916 - val_loss: 0.5702 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4587 - accuracy: 0.7917\n",
       "Epoch 186/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4235 - accuracy: 0.7959 - val_loss: 0.5582 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4536 - accuracy: 0.8034\n",
       "Epoch 187/250\n",
-      "691/691 [==============================] - 0s 102us/sample - loss: 0.4248 - accuracy: 0.7959 - val_loss: 0.5515 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4556 - accuracy: 0.7917\n",
       "Epoch 188/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4296 - accuracy: 0.7916 - val_loss: 0.6143 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.4555 - accuracy: 0.7943\n",
       "Epoch 189/250\n",
-      "691/691 [==============================] - 0s 115us/sample - loss: 0.4264 - accuracy: 0.7887 - val_loss: 0.5686 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.4515 - accuracy: 0.7956\n",
       "Epoch 190/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4224 - accuracy: 0.8003 - val_loss: 0.5524 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4507 - accuracy: 0.7969\n",
       "Epoch 191/250\n",
-      "691/691 [==============================] - 0s 103us/sample - loss: 0.4298 - accuracy: 0.7945 - val_loss: 0.5425 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4529 - accuracy: 0.7982\n",
       "Epoch 192/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4248 - accuracy: 0.7974 - val_loss: 0.5546 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4516 - accuracy: 0.7995\n",
       "Epoch 193/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4309 - accuracy: 0.7916 - val_loss: 0.6047 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.4513 - accuracy: 0.7930\n",
       "Epoch 194/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4375 - accuracy: 0.8003 - val_loss: 0.5604 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4482 - accuracy: 0.8034\n",
       "Epoch 195/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4221 - accuracy: 0.7959 - val_loss: 0.5587 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4515 - accuracy: 0.8008\n",
       "Epoch 196/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4223 - accuracy: 0.7902 - val_loss: 0.5423 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4522 - accuracy: 0.7917\n",
       "Epoch 197/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4304 - accuracy: 0.7916 - val_loss: 0.6116 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4507 - accuracy: 0.8021\n",
       "Epoch 198/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4300 - accuracy: 0.7815 - val_loss: 0.5754 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4501 - accuracy: 0.7969\n",
       "Epoch 199/250\n",
-      "691/691 [==============================] - 0s 116us/sample - loss: 0.4248 - accuracy: 0.7959 - val_loss: 0.5608 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4526 - accuracy: 0.8047\n",
       "Epoch 200/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4347 - accuracy: 0.7815 - val_loss: 0.5849 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4484 - accuracy: 0.7956\n",
       "Epoch 201/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4316 - accuracy: 0.7858 - val_loss: 0.6119 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.4486 - accuracy: 0.7995\n",
       "Epoch 202/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4332 - accuracy: 0.7902 - val_loss: 0.5676 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4471 - accuracy: 0.8060\n",
       "Epoch 203/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4353 - accuracy: 0.7742 - val_loss: 0.5462 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.4474 - accuracy: 0.8138\n",
       "Epoch 204/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4218 - accuracy: 0.7959 - val_loss: 0.5536 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4457 - accuracy: 0.8008\n",
       "Epoch 205/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4249 - accuracy: 0.7974 - val_loss: 0.5566 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4439 - accuracy: 0.8047\n",
       "Epoch 206/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4309 - accuracy: 0.7988 - val_loss: 0.6227 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4498 - accuracy: 0.8086\n",
       "Epoch 207/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4426 - accuracy: 0.8017 - val_loss: 0.5527 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.4469 - accuracy: 0.7982\n",
       "Epoch 208/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4240 - accuracy: 0.7945 - val_loss: 0.5741 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.4444 - accuracy: 0.7982\n",
       "Epoch 209/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4220 - accuracy: 0.8003 - val_loss: 0.5496 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4422 - accuracy: 0.8008\n",
       "Epoch 210/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4335 - accuracy: 0.7858 - val_loss: 0.5671 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4447 - accuracy: 0.7956\n",
       "Epoch 211/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4296 - accuracy: 0.7931 - val_loss: 0.5924 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4449 - accuracy: 0.7956\n",
       "Epoch 212/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4445 - accuracy: 0.7771 - val_loss: 0.5769 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4454 - accuracy: 0.7995\n",
       "Epoch 213/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4326 - accuracy: 0.7945 - val_loss: 0.5796 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.4441 - accuracy: 0.7995\n",
       "Epoch 214/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4225 - accuracy: 0.7974 - val_loss: 0.5894 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4426 - accuracy: 0.8099\n",
       "Epoch 215/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4219 - accuracy: 0.7988 - val_loss: 0.5632 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4445 - accuracy: 0.8008\n",
       "Epoch 216/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4329 - accuracy: 0.7844 - val_loss: 0.5483 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4436 - accuracy: 0.8034\n",
       "Epoch 217/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4232 - accuracy: 0.8032 - val_loss: 0.5585 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.4434 - accuracy: 0.8086\n",
       "Epoch 218/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4485 - accuracy: 0.7728 - val_loss: 0.5537 - val_accuracy: 0.8052\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.4419 - accuracy: 0.8060\n",
       "Epoch 219/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4373 - accuracy: 0.7988 - val_loss: 0.5409 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.4425 - accuracy: 0.8086\n",
       "Epoch 220/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4216 - accuracy: 0.8003 - val_loss: 0.5648 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.4410 - accuracy: 0.7982\n",
       "Epoch 221/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4290 - accuracy: 0.7988 - val_loss: 0.5723 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.4389 - accuracy: 0.8060\n",
       "Epoch 222/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4247 - accuracy: 0.7916 - val_loss: 0.5825 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.4407 - accuracy: 0.7917\n",
       "Epoch 223/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4257 - accuracy: 0.7931 - val_loss: 0.5462 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.4416 - accuracy: 0.8099\n",
       "Epoch 224/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4377 - accuracy: 0.7931 - val_loss: 0.5473 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.4384 - accuracy: 0.8060\n",
       "Epoch 225/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4260 - accuracy: 0.7988 - val_loss: 0.5481 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4405 - accuracy: 0.7982\n",
       "Epoch 226/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4254 - accuracy: 0.8003 - val_loss: 0.5611 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.4399 - accuracy: 0.7982\n",
       "Epoch 227/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4325 - accuracy: 0.7988 - val_loss: 0.5663 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4397 - accuracy: 0.8060\n",
       "Epoch 228/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4480 - accuracy: 0.7873 - val_loss: 0.5549 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4378 - accuracy: 0.8047\n",
       "Epoch 229/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4264 - accuracy: 0.7931 - val_loss: 0.5778 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.4339 - accuracy: 0.7956\n",
       "Epoch 230/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4295 - accuracy: 0.7931 - val_loss: 0.5351 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.4335 - accuracy: 0.8073\n",
       "Epoch 231/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4355 - accuracy: 0.7873 - val_loss: 0.5499 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.4342 - accuracy: 0.8099\n",
       "Epoch 232/250\n",
-      "691/691 [==============================] - 0s 122us/sample - loss: 0.4338 - accuracy: 0.7945 - val_loss: 0.5842 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.4364 - accuracy: 0.8060\n",
       "Epoch 233/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4278 - accuracy: 0.7945 - val_loss: 0.5997 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.4338 - accuracy: 0.7969\n",
       "Epoch 234/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4360 - accuracy: 0.7800 - val_loss: 0.5947 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4342 - accuracy: 0.8060\n",
       "Epoch 235/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4275 - accuracy: 0.7988 - val_loss: 0.5561 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.4349 - accuracy: 0.7969\n",
       "Epoch 236/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4229 - accuracy: 0.7902 - val_loss: 0.5543 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4351 - accuracy: 0.8008\n",
       "Epoch 237/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4232 - accuracy: 0.7887 - val_loss: 0.5724 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.4347 - accuracy: 0.8073\n",
       "Epoch 238/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4278 - accuracy: 0.8003 - val_loss: 0.5673 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4356 - accuracy: 0.7982\n",
       "Epoch 239/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4350 - accuracy: 0.8017 - val_loss: 0.5767 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4361 - accuracy: 0.7969\n",
       "Epoch 240/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4240 - accuracy: 0.7844 - val_loss: 0.5844 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.4340 - accuracy: 0.7995\n",
       "Epoch 241/250\n",
-      "691/691 [==============================] - 0s 115us/sample - loss: 0.4246 - accuracy: 0.7988 - val_loss: 0.5608 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4367 - accuracy: 0.7982\n",
       "Epoch 242/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4225 - accuracy: 0.8017 - val_loss: 0.5395 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.4312 - accuracy: 0.8034\n",
       "Epoch 243/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4318 - accuracy: 0.7974 - val_loss: 0.5513 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.4383 - accuracy: 0.8086\n",
       "Epoch 244/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4237 - accuracy: 0.7974 - val_loss: 0.5530 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.4283 - accuracy: 0.8060\n",
       "Epoch 245/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4369 - accuracy: 0.7887 - val_loss: 0.5769 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.4366 - accuracy: 0.7982\n",
       "Epoch 246/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4223 - accuracy: 0.7858 - val_loss: 0.5797 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4329 - accuracy: 0.8021\n",
       "Epoch 247/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4224 - accuracy: 0.7945 - val_loss: 0.5757 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.4301 - accuracy: 0.8008\n",
       "Epoch 248/250\n",
-      "691/691 [==============================] - 0s 119us/sample - loss: 0.4231 - accuracy: 0.7988 - val_loss: 0.5811 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.4352 - accuracy: 0.8073\n",
       "Epoch 249/250\n",
-      "691/691 [==============================] - 0s 115us/sample - loss: 0.4202 - accuracy: 0.7988 - val_loss: 0.5648 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.4305 - accuracy: 0.8060\n",
       "Epoch 250/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4216 - accuracy: 0.7988 - val_loss: 0.5674 - val_accuracy: 0.7013\n"
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.4321 - accuracy: 0.8047\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7fafff74d048>"
+       "<tensorflow.python.keras.callbacks.History at 0x186cefbcd08>"
       ]
      },
-     "execution_count": 37,
+     "execution_count": 78,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
-    "model_improved.fit(X,y, epochs=250, validation_split=.10)"
+    "model_improved.fit(X,y, epochs=250)\n",
+    "# 70%"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 34,
+   "execution_count": 79,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "768/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 161us/sample - loss: 0.4706 - accuracy: 0.7630\n"
+      "Model: \"Nadamclipnorm\"\n",
+      "_________________________________________________________________\n",
+      "Layer (type)                 Output Shape              Param #   \n",
+      "=================================================================\n",
+      "dense_69 (Dense)             (None, 8)                 72        \n",
+      "_________________________________________________________________\n",
+      "dense_70 (Dense)             (None, 8)                 72        \n",
+      "_________________________________________________________________\n",
+      "dense_71 (Dense)             (None, 3)                 27        \n",
+      "_________________________________________________________________\n",
+      "dense_72 (Dense)             (None, 3)                 12        \n",
+      "_________________________________________________________________\n",
+      "dense_73 (Dense)             (None, 1)                 4         \n",
+      "=================================================================\n",
+      "Total params: 187\n",
+      "Trainable params: 187\n",
+      "Non-trainable params: 0\n",
+      "_________________________________________________________________\n"
+     ]
+    }
+   ],
+   "source": [
+    "from sklearn.preprocessing import Normalizer\n",
+    "\n",
+    "norm = Normalizer()\n",
+    "X_n = norm.fit_transform(X)\n",
+    "\n",
+    "nadam = Nadam(clipnorm=.7)\n",
+    "\n",
+    "model_improved = Sequential(name=\"Nadamclipnorm\")\n",
+    "\n",
+    "model_improved.add(Dense(8, input_dim=8, activation='relu'))\n",
+    "model_improved.add(Dense(8, activation='relu'))\n",
+    "model_improved.add(Dense(3, activation='relu'))\n",
+    "model_improved.add(Dense(3, activation='relu'))\n",
+    "model_improved.add(Dense(1, activation='sigmoid'))\n",
+    "\n",
+    "model_improved.compile(loss='binary_crossentropy', optimizer=nadam,\n",
+    "              metrics=['accuracy'])\n",
+    "\n",
+    "# Let's inspect our new architecture\n",
+    "model_improved.summary()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 80,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Train on 768 samples\n",
+      "Epoch 1/250\n",
+      "768/768 [==============================] - 1s 704us/sample - loss: 0.6817 - accuracy: 0.6510\n",
+      "Epoch 2/250\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6725 - accuracy: 0.6510\n",
+      "Epoch 3/250\n",
+      "768/768 [==============================] - 0s 57us/sample - loss: 0.6616 - accuracy: 0.6510\n",
+      "Epoch 4/250\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6504 - accuracy: 0.6510\n",
+      "Epoch 5/250\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6436 - accuracy: 0.6510\n",
+      "Epoch 6/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6412 - accuracy: 0.6510\n",
+      "Epoch 7/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6403 - accuracy: 0.6510\n",
+      "Epoch 8/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6398 - accuracy: 0.6510\n",
+      "Epoch 9/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6393 - accuracy: 0.6510\n",
+      "Epoch 10/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6388 - accuracy: 0.6510\n",
+      "Epoch 11/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6384 - accuracy: 0.6510\n",
+      "Epoch 12/250\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6377 - accuracy: 0.6510\n",
+      "Epoch 13/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6371 - accuracy: 0.6510\n",
+      "Epoch 14/250\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6368 - accuracy: 0.6510\n",
+      "Epoch 15/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6358 - accuracy: 0.6510\n",
+      "Epoch 16/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6351 - accuracy: 0.6510\n",
+      "Epoch 17/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6343 - accuracy: 0.6510\n",
+      "Epoch 18/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6336 - accuracy: 0.6510\n",
+      "Epoch 19/250\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6330 - accuracy: 0.6510\n",
+      "Epoch 20/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6319 - accuracy: 0.6510\n",
+      "Epoch 21/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6312 - accuracy: 0.6510\n",
+      "Epoch 22/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6306 - accuracy: 0.6510\n",
+      "Epoch 23/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6292 - accuracy: 0.6510\n",
+      "Epoch 24/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6282 - accuracy: 0.6510\n",
+      "Epoch 25/250\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6268 - accuracy: 0.6510\n",
+      "Epoch 26/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6259 - accuracy: 0.6510\n",
+      "Epoch 27/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6246 - accuracy: 0.6510\n",
+      "Epoch 28/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6233 - accuracy: 0.6510\n",
+      "Epoch 29/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6219 - accuracy: 0.6510\n",
+      "Epoch 30/250\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6206 - accuracy: 0.6510\n",
+      "Epoch 31/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6193 - accuracy: 0.6510\n",
+      "Epoch 32/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6176 - accuracy: 0.6510\n",
+      "Epoch 33/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6163 - accuracy: 0.6510\n",
+      "Epoch 34/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6153 - accuracy: 0.6510\n",
+      "Epoch 35/250\n",
+      "768/768 [==============================] - 0s 72us/sample - loss: 0.6133 - accuracy: 0.6510\n",
+      "Epoch 36/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6114 - accuracy: 0.6510\n",
+      "Epoch 37/250\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6108 - accuracy: 0.6510\n",
+      "Epoch 38/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6084 - accuracy: 0.6510\n",
+      "Epoch 39/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6081 - accuracy: 0.6510\n",
+      "Epoch 40/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6068 - accuracy: 0.6510\n",
+      "Epoch 41/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6049 - accuracy: 0.6510\n",
+      "Epoch 42/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6039 - accuracy: 0.6497\n",
+      "Epoch 43/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6026 - accuracy: 0.6458\n",
+      "Epoch 44/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.6014 - accuracy: 0.6497\n",
+      "Epoch 45/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6004 - accuracy: 0.6549\n",
+      "Epoch 46/250\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.5999 - accuracy: 0.6510\n",
+      "Epoch 47/250\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.5987 - accuracy: 0.6510\n",
+      "Epoch 48/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5973 - accuracy: 0.6549\n",
+      "Epoch 49/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5964 - accuracy: 0.6602\n",
+      "Epoch 50/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5957 - accuracy: 0.6602\n",
+      "Epoch 51/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5950 - accuracy: 0.6615\n",
+      "Epoch 52/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5949 - accuracy: 0.6536\n",
+      "Epoch 53/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5933 - accuracy: 0.6641\n",
+      "Epoch 54/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5929 - accuracy: 0.6667\n",
+      "Epoch 55/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5918 - accuracy: 0.6745\n",
+      "Epoch 56/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5917 - accuracy: 0.6771\n",
+      "Epoch 57/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5910 - accuracy: 0.6862\n",
+      "Epoch 58/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5910 - accuracy: 0.6797\n",
+      "Epoch 59/250\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.5899 - accuracy: 0.6745\n",
+      "Epoch 60/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5896 - accuracy: 0.6849\n",
+      "Epoch 61/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5883 - accuracy: 0.6836\n",
+      "Epoch 62/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5881 - accuracy: 0.6875\n",
+      "Epoch 63/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5887 - accuracy: 0.6888\n",
+      "Epoch 64/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5876 - accuracy: 0.6823\n",
+      "Epoch 65/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5867 - accuracy: 0.6836\n",
+      "Epoch 66/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5862 - accuracy: 0.6953\n",
+      "Epoch 67/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5859 - accuracy: 0.6862\n",
+      "Epoch 68/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5852 - accuracy: 0.6849\n",
+      "Epoch 69/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5850 - accuracy: 0.6914\n",
+      "Epoch 70/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5844 - accuracy: 0.6901\n",
+      "Epoch 71/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5849 - accuracy: 0.6940\n",
+      "Epoch 72/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5838 - accuracy: 0.6914\n",
+      "Epoch 73/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5833 - accuracy: 0.6979\n",
+      "Epoch 74/250\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.5829 - accuracy: 0.6901\n",
+      "Epoch 75/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5826 - accuracy: 0.6927\n",
+      "Epoch 76/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5820 - accuracy: 0.6940\n",
+      "Epoch 77/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5825 - accuracy: 0.6836\n",
+      "Epoch 78/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5824 - accuracy: 0.6862\n",
+      "Epoch 79/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5823 - accuracy: 0.6953\n",
+      "Epoch 80/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5811 - accuracy: 0.6940\n",
+      "Epoch 81/250\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.5812 - accuracy: 0.6914\n",
+      "Epoch 82/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5810 - accuracy: 0.6914\n",
+      "Epoch 83/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5808 - accuracy: 0.6966\n",
+      "Epoch 84/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5821 - accuracy: 0.6927\n",
+      "Epoch 85/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5798 - accuracy: 0.6966\n",
+      "Epoch 86/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5795 - accuracy: 0.6927\n",
+      "Epoch 87/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5798 - accuracy: 0.6836\n",
+      "Epoch 88/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5795 - accuracy: 0.6966\n",
+      "Epoch 89/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5793 - accuracy: 0.6966\n",
+      "Epoch 90/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5790 - accuracy: 0.6979\n",
+      "Epoch 91/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5794 - accuracy: 0.6927\n",
+      "Epoch 92/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5785 - accuracy: 0.6914\n",
+      "Epoch 93/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5792 - accuracy: 0.6914\n",
+      "Epoch 94/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5786 - accuracy: 0.6966\n",
+      "Epoch 95/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5777 - accuracy: 0.6927\n",
+      "Epoch 96/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5782 - accuracy: 0.6992\n",
+      "Epoch 97/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5772 - accuracy: 0.6953\n",
+      "Epoch 98/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5780 - accuracy: 0.6862\n",
+      "Epoch 99/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5776 - accuracy: 0.6966\n",
+      "Epoch 100/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5775 - accuracy: 0.6940\n",
+      "Epoch 101/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5781 - accuracy: 0.6940\n",
+      "Epoch 102/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5771 - accuracy: 0.6901\n",
+      "Epoch 103/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5766 - accuracy: 0.6914\n",
+      "Epoch 104/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5771 - accuracy: 0.6992\n",
+      "Epoch 105/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5765 - accuracy: 0.6940\n",
+      "Epoch 106/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5770 - accuracy: 0.6966\n",
+      "Epoch 107/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5764 - accuracy: 0.6927\n",
+      "Epoch 108/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5758 - accuracy: 0.6940\n",
+      "Epoch 109/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5766 - accuracy: 0.6927\n",
+      "Epoch 110/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5757 - accuracy: 0.6966\n",
+      "Epoch 111/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5757 - accuracy: 0.6992\n",
+      "Epoch 112/250\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.5754 - accuracy: 0.6927\n",
+      "Epoch 113/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5756 - accuracy: 0.6992\n",
+      "Epoch 114/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5756 - accuracy: 0.6953\n",
+      "Epoch 115/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5758 - accuracy: 0.6940\n",
+      "Epoch 116/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5760 - accuracy: 0.6927\n",
+      "Epoch 117/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5749 - accuracy: 0.6953\n",
+      "Epoch 118/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5743 - accuracy: 0.7005\n",
+      "Epoch 119/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5751 - accuracy: 0.6940\n",
+      "Epoch 120/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5750 - accuracy: 0.6953\n",
+      "Epoch 121/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5744 - accuracy: 0.6966\n",
+      "Epoch 122/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5744 - accuracy: 0.6940\n",
+      "Epoch 123/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5749 - accuracy: 0.7044\n",
+      "Epoch 124/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5749 - accuracy: 0.6940\n",
+      "Epoch 125/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5739 - accuracy: 0.7005\n",
+      "Epoch 126/250\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.5746 - accuracy: 0.6992\n",
+      "Epoch 127/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5755 - accuracy: 0.7005\n",
+      "Epoch 128/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5740 - accuracy: 0.7057\n",
+      "Epoch 129/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5736 - accuracy: 0.6953\n",
+      "Epoch 130/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5736 - accuracy: 0.6927\n",
+      "Epoch 131/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5733 - accuracy: 0.6953\n",
+      "Epoch 132/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5731 - accuracy: 0.6966\n",
+      "Epoch 133/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5742 - accuracy: 0.6927\n",
+      "Epoch 134/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5734 - accuracy: 0.7031\n",
+      "Epoch 135/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5731 - accuracy: 0.6914\n",
+      "Epoch 136/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5731 - accuracy: 0.6940\n",
+      "Epoch 137/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5724 - accuracy: 0.6914\n",
+      "Epoch 138/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5727 - accuracy: 0.6966\n",
+      "Epoch 139/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5728 - accuracy: 0.6940\n",
+      "Epoch 140/250\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.5729 - accuracy: 0.6966\n",
+      "Epoch 141/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5724 - accuracy: 0.7005\n",
+      "Epoch 142/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5725 - accuracy: 0.6888\n",
+      "Epoch 143/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5719 - accuracy: 0.6953\n",
+      "Epoch 144/250\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.5720 - accuracy: 0.6953\n",
+      "Epoch 145/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5714 - accuracy: 0.6940\n",
+      "Epoch 146/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5713 - accuracy: 0.6940\n",
+      "Epoch 147/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5712 - accuracy: 0.6979\n",
+      "Epoch 148/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5709 - accuracy: 0.6953\n",
+      "Epoch 149/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5709 - accuracy: 0.6914\n",
+      "Epoch 150/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5703 - accuracy: 0.6927\n",
+      "Epoch 151/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5703 - accuracy: 0.6940\n",
+      "Epoch 152/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5708 - accuracy: 0.6901\n",
+      "Epoch 153/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5706 - accuracy: 0.6966\n",
+      "Epoch 154/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5703 - accuracy: 0.6940\n",
+      "Epoch 155/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5701 - accuracy: 0.6979\n",
+      "Epoch 156/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5701 - accuracy: 0.7005\n",
+      "Epoch 157/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5696 - accuracy: 0.6966\n",
+      "Epoch 158/250\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.5703 - accuracy: 0.6940\n",
+      "Epoch 159/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5685 - accuracy: 0.6979\n",
+      "Epoch 160/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5702 - accuracy: 0.6940\n",
+      "Epoch 161/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5694 - accuracy: 0.6914\n",
+      "Epoch 162/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5703 - accuracy: 0.6953\n",
+      "Epoch 163/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5691 - accuracy: 0.6953\n",
+      "Epoch 164/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5688 - accuracy: 0.6992\n",
+      "Epoch 165/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5687 - accuracy: 0.6953\n",
+      "Epoch 166/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5688 - accuracy: 0.6979\n",
+      "Epoch 167/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5691 - accuracy: 0.7005\n",
+      "Epoch 168/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5689 - accuracy: 0.6927\n",
+      "Epoch 169/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5679 - accuracy: 0.6979\n",
+      "Epoch 170/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5683 - accuracy: 0.7005\n",
+      "Epoch 171/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5688 - accuracy: 0.6940\n",
+      "Epoch 172/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5686 - accuracy: 0.6940\n",
+      "Epoch 173/250\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.5679 - accuracy: 0.6992\n",
+      "Epoch 174/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5678 - accuracy: 0.6927\n",
+      "Epoch 175/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5678 - accuracy: 0.7018\n",
+      "Epoch 176/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5669 - accuracy: 0.6992\n",
+      "Epoch 177/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5676 - accuracy: 0.6979\n",
+      "Epoch 178/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5676 - accuracy: 0.6966\n",
+      "Epoch 179/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5673 - accuracy: 0.6966\n",
+      "Epoch 180/250\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.5673 - accuracy: 0.6953\n",
+      "Epoch 181/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5678 - accuracy: 0.6979\n",
+      "Epoch 182/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5671 - accuracy: 0.6979\n",
+      "Epoch 183/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5673 - accuracy: 0.6940\n",
+      "Epoch 184/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5668 - accuracy: 0.7005\n",
+      "Epoch 185/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5675 - accuracy: 0.6953\n",
+      "Epoch 186/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5671 - accuracy: 0.6953\n",
+      "Epoch 187/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5664 - accuracy: 0.6992\n",
+      "Epoch 188/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5677 - accuracy: 0.6966\n",
+      "Epoch 189/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5662 - accuracy: 0.6966\n",
+      "Epoch 190/250\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.5668 - accuracy: 0.6966\n",
+      "Epoch 191/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5668 - accuracy: 0.6940\n",
+      "Epoch 192/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5664 - accuracy: 0.6979\n",
+      "Epoch 193/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5665 - accuracy: 0.6966\n",
+      "Epoch 194/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5666 - accuracy: 0.6953\n",
+      "Epoch 195/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5666 - accuracy: 0.7005\n",
+      "Epoch 196/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5670 - accuracy: 0.7057\n",
+      "Epoch 197/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5667 - accuracy: 0.6992\n",
+      "Epoch 198/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5661 - accuracy: 0.7018\n",
+      "Epoch 199/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5662 - accuracy: 0.6992\n",
+      "Epoch 200/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5657 - accuracy: 0.7005\n",
+      "Epoch 201/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5660 - accuracy: 0.6953\n",
+      "Epoch 202/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5665 - accuracy: 0.6979\n",
+      "Epoch 203/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5664 - accuracy: 0.6953\n",
+      "Epoch 204/250\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.5655 - accuracy: 0.6979\n",
+      "Epoch 205/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5654 - accuracy: 0.7018\n",
+      "Epoch 206/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5664 - accuracy: 0.7018\n",
+      "Epoch 207/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5655 - accuracy: 0.7070\n",
+      "Epoch 208/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5654 - accuracy: 0.6979\n",
+      "Epoch 209/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5656 - accuracy: 0.7005\n",
+      "Epoch 210/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5650 - accuracy: 0.6966\n",
+      "Epoch 211/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5654 - accuracy: 0.7018\n",
+      "Epoch 212/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5652 - accuracy: 0.7005\n",
+      "Epoch 213/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5657 - accuracy: 0.6979\n",
+      "Epoch 214/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5650 - accuracy: 0.7018\n",
+      "Epoch 215/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5654 - accuracy: 0.6992\n",
+      "Epoch 216/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5656 - accuracy: 0.6992\n",
+      "Epoch 217/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5650 - accuracy: 0.6979\n",
+      "Epoch 218/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5652 - accuracy: 0.7018\n",
+      "Epoch 219/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5644 - accuracy: 0.7070\n",
+      "Epoch 220/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5652 - accuracy: 0.7057\n",
+      "Epoch 221/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5654 - accuracy: 0.6953\n",
+      "Epoch 222/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5645 - accuracy: 0.7044\n",
+      "Epoch 223/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5655 - accuracy: 0.7031\n",
+      "Epoch 224/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5648 - accuracy: 0.7057\n",
+      "Epoch 225/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5648 - accuracy: 0.6966\n",
+      "Epoch 226/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5643 - accuracy: 0.7005\n",
+      "Epoch 227/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5646 - accuracy: 0.7018\n",
+      "Epoch 228/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5650 - accuracy: 0.7005\n",
+      "Epoch 229/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5646 - accuracy: 0.6979\n",
+      "Epoch 230/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5643 - accuracy: 0.7057\n",
+      "Epoch 231/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5644 - accuracy: 0.7005\n",
+      "Epoch 232/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5645 - accuracy: 0.7018\n",
+      "Epoch 233/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5643 - accuracy: 0.7057\n",
+      "Epoch 234/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5637 - accuracy: 0.7044\n",
+      "Epoch 235/250\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 0.5654 - accuracy: 0.6927\n",
+      "Epoch 236/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5643 - accuracy: 0.7044\n",
+      "Epoch 237/250\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 0.5646 - accuracy: 0.6966\n",
+      "Epoch 238/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5644 - accuracy: 0.6966\n",
+      "Epoch 239/250\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5648 - accuracy: 0.7031\n",
+      "Epoch 240/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5641 - accuracy: 0.6992\n",
+      "Epoch 241/250\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.5641 - accuracy: 0.7083\n",
+      "Epoch 242/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5641 - accuracy: 0.7031\n",
+      "Epoch 243/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5635 - accuracy: 0.7018\n",
+      "Epoch 244/250\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.5639 - accuracy: 0.7031\n",
+      "Epoch 245/250\n",
+      "768/768 [==============================] - 0s 66us/sample - loss: 0.5646 - accuracy: 0.6992\n",
+      "Epoch 246/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5641 - accuracy: 0.7070\n",
+      "Epoch 247/250\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.5636 - accuracy: 0.6992\n",
+      "Epoch 248/250\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.5635 - accuracy: 0.7083\n",
+      "Epoch 249/250\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.5639 - accuracy: 0.7005\n",
+      "Epoch 250/250\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.5641 - accuracy: 0.7096\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "[0.4735589859386285, 0.7630208]"
+       "<tensorflow.python.keras.callbacks.History at 0x186d0216408>"
       ]
      },
-     "execution_count": 34,
+     "execution_count": 80,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
-    "model_improved.evaluate(X,y)\n",
-    "#print(f\"{model_improved.metrics_names[1]}: {scores[1]*100}\")"
+    "model_improved.fit(X_n,y, epochs=250)\n",
+    "# 73%"
    ]
   },
   {
@@ -1942,7 +2608,10 @@
     "\n",
     "ReLU stands for Rectified Linear Units it is by far the most commonly used activation function in modern neural networks. It doesn't activate neurons that are being passed a negative signal and passes on positive signals. Think about why this might be useful. Remember how a lot of our initial weights got set to negative numbers by chance? This would have dealt with those negative weights a lot faster than the sigmoid function updating. What does the derivative of this function look like? It looks like the step function! This means that not all neurons are activated. With sigmoid basically all of our neurons are passing some amount of signal even if it's small making it hard for the network to differentiate important and less important connections. ReLU turns off a portion of our less important neurons which decreases computational load, but also helps the network learn what the most important connections are faster. \n",
     "\n",
-    "What's the problem with relu? Well the left half of its derivative function shows that for neurons that are initialized with weights that cause them to have no activation, our gradient will not update those neuron's weights, this can lead to dead neurons that never fire and whose weights never get updated. We would probably want to update the weights of neurons that didn't fire even if it's just by a little bit in case we got unlucky with our initial weights and want to give those neurons a chance of turning back on in the future."
+    "What's the problem with relu? Well the left half of its derivative function shows that for neurons that are initialized with weights that cause them to have no activation, our gradient will not update those neuron's weights, this can lead to dead neurons that never fire and whose weights never get updated. We would probably want to update the weights of neurons that didn't fire even if it's just by a little bit in case we got unlucky with our initial weights and want to give those neurons a chance of turning back on in the future.\n",
+    "\n",
+    "\n",
+    "> one reason its so popular is it will basically stop the model from updating negative weights, killing neurons that arent useful, making everything faster"
    ]
   },
   {
@@ -2003,7 +2672,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 38,
+   "execution_count": 81,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -2024,7 +2693,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 39,
+   "execution_count": 82,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -2036,7 +2705,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 40,
+   "execution_count": 83,
    "metadata": {},
    "outputs": [
     {
@@ -2044,7 +2713,7 @@
      "output_type": "stream",
      "text": [
       "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
-      "11493376/11490434 [==============================] - 0s 0us/step\n"
+      "11493376/11490434 [==============================] - 1s 0us/step\n"
      ]
     }
    ],
@@ -2055,7 +2724,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 41,
+   "execution_count": 84,
    "metadata": {},
    "outputs": [
     {
@@ -2064,7 +2733,7 @@
        "(28, 28)"
       ]
      },
-     "execution_count": 41,
+     "execution_count": 84,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -2075,25 +2744,130 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 85,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "(60000, 28, 28)"
+      ]
+     },
+     "execution_count": 85,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
    "source": [
     "X_train.shape"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 86,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
+       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
+       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
+       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
+       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
+       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
+       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
+       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
+       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
+       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
+       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
+       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
+       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
+       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
+       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0]], dtype=uint8)"
+      ]
+     },
+     "execution_count": 86,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
    "source": [
     "X_train[0]"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 42,
+   "execution_count": 87,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -2104,7 +2878,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 43,
+   "execution_count": 88,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -2115,7 +2889,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 44,
+   "execution_count": 89,
    "metadata": {},
    "outputs": [
     {
@@ -2124,7 +2898,7 @@
        "4"
       ]
      },
-     "execution_count": 44,
+     "execution_count": 89,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -2135,7 +2909,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 45,
+   "execution_count": 90,
    "metadata": {},
    "outputs": [
     {
@@ -2144,7 +2918,7 @@
        "4"
       ]
      },
-     "execution_count": 45,
+     "execution_count": 90,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -2155,7 +2929,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 47,
+   "execution_count": 91,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -2168,7 +2942,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 48,
+   "execution_count": 92,
    "metadata": {},
    "outputs": [
     {
@@ -2177,7 +2951,7 @@
        "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)"
       ]
      },
-     "execution_count": 48,
+     "execution_count": 92,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -2188,26 +2962,26 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 49,
+   "execution_count": 93,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Model: \"sequential_3\"\n",
+      "Model: \"sequential_5\"\n",
       "_________________________________________________________________\n",
       "Layer (type)                 Output Shape              Param #   \n",
       "=================================================================\n",
-      "dense_14 (Dense)             (None, 16)                12560     \n",
+      "dense_74 (Dense)             (None, 16)                12560     \n",
       "_________________________________________________________________\n",
-      "dense_15 (Dense)             (None, 16)                272       \n",
+      "dense_75 (Dense)             (None, 16)                272       \n",
       "_________________________________________________________________\n",
-      "dense_16 (Dense)             (None, 16)                272       \n",
+      "dense_76 (Dense)             (None, 16)                272       \n",
       "_________________________________________________________________\n",
-      "dense_17 (Dense)             (None, 16)                272       \n",
+      "dense_77 (Dense)             (None, 16)                272       \n",
       "_________________________________________________________________\n",
-      "dense_18 (Dense)             (None, 10)                170       \n",
+      "dense_78 (Dense)             (None, 10)                170       \n",
       "=================================================================\n",
       "Total params: 13,546\n",
       "Trainable params: 13,546\n",
@@ -2240,7 +3014,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 94,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -2255,54 +3029,62 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 95,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "12544"
+      ]
+     },
+     "execution_count": 95,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
    "source": [
     "16 *  784"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 96,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "(10000, 10)"
+      ]
+     },
+     "execution_count": 96,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
    "source": [
     "y_test.shape"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 50,
+   "execution_count": 97,
    "metadata": {},
    "outputs": [
     {
-     "ename": "KeyboardInterrupt",
-     "evalue": "",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
-      "\u001b[0;32m<ipython-input-50-fd0808510929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(f'{mnist_model.metrics_names[1]}: {scores[1]*100}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.2851 - accuracy: 0.9243\n",
+      "accuracy: 92.43000149726868\n"
      ]
     }
    ],
    "source": [
     "history = mnist_model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=False)\n",
     "scores = mnist_model.evaluate(X_test, y_test)\n",
-    "#print(f'{mnist_model.metrics_names[1]}: {scores[1]*100}')"
+    "print(f'{mnist_model.metrics_names[1]}: {scores[1]*100}')"
    ]
   },
   {
@@ -2390,7 +3172,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.6.8"
+   "version": "3.7.4"
   },
   "toc-autonumbering": false,
   "toc-showmarkdowntxt": false
diff --git a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
index ca65dc6..cf51878 100644
--- a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
+++ b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
@@ -91,9 +91,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.7.4"
   }
  },
  "nbformat": 4,
- "nbformat_minor": 2
+ "nbformat_minor": 4
 }
diff --git a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
index 4bb18e9..9b91aaa 100644
--- a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
+++ b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
@@ -81,7 +81,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -106,7 +106,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 4,
    "metadata": {},
    "outputs": [
     {
@@ -170,7 +170,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 5,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -187,164 +187,164 @@
      "text": [
       "Train on 404 samples, validate on 102 samples\n",
       "Epoch 1/75\n",
-      "404/404 [==============================] - 2s 4ms/sample - loss: 498.2045 - mse: 498.2046 - mae: 20.2543 - val_loss: 421.5039 - val_mse: 421.5038 - val_mae: 18.3349\n",
+      "404/404 [==============================] - 0s 1ms/sample - loss: 507.7091 - mse: 507.7090 - mae: 20.3634 - val_loss: 426.9547 - val_mse: 426.9547 - val_mae: 18.4208\n",
       "Epoch 2/75\n",
-      "404/404 [==============================] - 0s 347us/sample - loss: 249.6985 - mse: 249.6985 - mae: 13.2672 - val_loss: 111.3743 - val_mse: 111.3743 - val_mae: 8.6210\n",
+      "404/404 [==============================] - 0s 205us/sample - loss: 256.9884 - mse: 256.9884 - mae: 13.5340 - val_loss: 106.3865 - val_mse: 106.3865 - val_mae: 8.9428\n",
       "Epoch 3/75\n",
-      "404/404 [==============================] - 0s 344us/sample - loss: 56.6755 - mse: 56.6755 - mae: 5.4817 - val_loss: 39.1997 - val_mse: 39.1997 - val_mae: 4.9872\n",
+      "404/404 [==============================] - 0s 225us/sample - loss: 54.3487 - mse: 54.3487 - mae: 5.4886 - val_loss: 32.4865 - val_mse: 32.4865 - val_mae: 4.5770\n",
       "Epoch 4/75\n",
-      "404/404 [==============================] - 0s 364us/sample - loss: 28.3243 - mse: 28.3243 - mae: 3.7054 - val_loss: 26.9866 - val_mse: 26.9866 - val_mae: 4.0796\n",
+      "404/404 [==============================] - 0s 228us/sample - loss: 27.6369 - mse: 27.6369 - mae: 3.7288 - val_loss: 26.6257 - val_mse: 26.6257 - val_mae: 4.0145\n",
       "Epoch 5/75\n",
-      "404/404 [==============================] - 0s 382us/sample - loss: 20.5281 - mse: 20.5281 - mae: 3.1209 - val_loss: 24.6172 - val_mse: 24.6172 - val_mae: 3.8052\n",
+      "404/404 [==============================] - 0s 220us/sample - loss: 22.0352 - mse: 22.0352 - mae: 3.3883 - val_loss: 24.8551 - val_mse: 24.8552 - val_mae: 3.8195\n",
       "Epoch 6/75\n",
-      "404/404 [==============================] - 0s 393us/sample - loss: 17.9283 - mse: 17.9283 - mae: 2.8665 - val_loss: 23.6524 - val_mse: 23.6524 - val_mae: 3.6746\n",
+      "404/404 [==============================] - 0s 228us/sample - loss: 19.3214 - mse: 19.3214 - mae: 3.1142 - val_loss: 23.8226 - val_mse: 23.8226 - val_mae: 3.6988\n",
       "Epoch 7/75\n",
-      "404/404 [==============================] - 0s 440us/sample - loss: 16.9179 - mse: 16.9179 - mae: 2.8781 - val_loss: 23.4620 - val_mse: 23.4620 - val_mae: 3.5778\n",
+      "404/404 [==============================] - 0s 220us/sample - loss: 17.5861 - mse: 17.5861 - mae: 2.9929 - val_loss: 24.2517 - val_mse: 24.2517 - val_mae: 3.6656\n",
       "Epoch 8/75\n",
-      "404/404 [==============================] - 0s 366us/sample - loss: 15.1579 - mse: 15.1579 - mae: 2.6440 - val_loss: 24.1374 - val_mse: 24.1374 - val_mae: 3.5929\n",
+      "404/404 [==============================] - 0s 203us/sample - loss: 16.2669 - mse: 16.2669 - mae: 2.9000 - val_loss: 24.2651 - val_mse: 24.2651 - val_mae: 3.6321\n",
       "Epoch 9/75\n",
-      "404/404 [==============================] - 0s 367us/sample - loss: 14.1717 - mse: 14.1717 - mae: 2.5937 - val_loss: 24.4829 - val_mse: 24.4829 - val_mae: 3.5639\n",
+      "404/404 [==============================] - 0s 225us/sample - loss: 15.0246 - mse: 15.0246 - mae: 2.7867 - val_loss: 22.5821 - val_mse: 22.5821 - val_mae: 3.4352\n",
       "Epoch 10/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 13.5002 - mse: 13.5002 - mae: 2.5633 - val_loss: 25.0170 - val_mse: 25.0170 - val_mae: 3.5601\n",
+      "404/404 [==============================] - 0s 193us/sample - loss: 14.0367 - mse: 14.0367 - mae: 2.6846 - val_loss: 23.7128 - val_mse: 23.7128 - val_mae: 3.4865\n",
       "Epoch 11/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 12.8641 - mse: 12.8641 - mae: 2.4963 - val_loss: 25.1162 - val_mse: 25.1162 - val_mae: 3.5449\n",
+      "404/404 [==============================] - 0s 163us/sample - loss: 13.3936 - mse: 13.3936 - mae: 2.6309 - val_loss: 23.3635 - val_mse: 23.3635 - val_mae: 3.4370\n",
       "Epoch 12/75\n",
-      "404/404 [==============================] - 0s 351us/sample - loss: 12.4033 - mse: 12.4033 - mae: 2.5224 - val_loss: 25.0382 - val_mse: 25.0382 - val_mae: 3.4858\n",
+      "404/404 [==============================] - 0s 166us/sample - loss: 12.5320 - mse: 12.5320 - mae: 2.5608 - val_loss: 23.2658 - val_mse: 23.2658 - val_mae: 3.3570\n",
       "Epoch 13/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 12.2653 - mse: 12.2653 - mae: 2.4637 - val_loss: 26.7274 - val_mse: 26.7274 - val_mae: 3.6054\n",
+      "404/404 [==============================] - 0s 186us/sample - loss: 11.9970 - mse: 11.9970 - mae: 2.4660 - val_loss: 22.8869 - val_mse: 22.8869 - val_mae: 3.2973\n",
       "Epoch 14/75\n",
-      "404/404 [==============================] - 0s 368us/sample - loss: 11.8249 - mse: 11.8249 - mae: 2.4648 - val_loss: 25.2347 - val_mse: 25.2347 - val_mae: 3.4602\n",
+      "404/404 [==============================] - 0s 168us/sample - loss: 11.6773 - mse: 11.6773 - mae: 2.4712 - val_loss: 23.7416 - val_mse: 23.7416 - val_mae: 3.3228\n",
       "Epoch 15/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 11.3965 - mse: 11.3965 - mae: 2.4134 - val_loss: 25.3070 - val_mse: 25.3070 - val_mae: 3.4305\n",
+      "404/404 [==============================] - 0s 203us/sample - loss: 11.1049 - mse: 11.1049 - mae: 2.3802 - val_loss: 22.6889 - val_mse: 22.6889 - val_mae: 3.2282\n",
       "Epoch 16/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 11.0982 - mse: 11.0982 - mae: 2.3616 - val_loss: 25.0599 - val_mse: 25.0599 - val_mae: 3.3784\n",
+      "404/404 [==============================] - 0s 163us/sample - loss: 10.8064 - mse: 10.8064 - mae: 2.3594 - val_loss: 22.5639 - val_mse: 22.5639 - val_mae: 3.2066\n",
       "Epoch 17/75\n",
-      "404/404 [==============================] - 0s 365us/sample - loss: 11.1969 - mse: 11.1969 - mae: 2.3806 - val_loss: 25.1976 - val_mse: 25.1976 - val_mae: 3.3732\n",
+      "404/404 [==============================] - 0s 191us/sample - loss: 10.4634 - mse: 10.4634 - mae: 2.3495 - val_loss: 25.1876 - val_mse: 25.1876 - val_mae: 3.3717\n",
       "Epoch 18/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 10.9278 - mse: 10.9278 - mae: 2.3653 - val_loss: 24.2875 - val_mse: 24.2875 - val_mae: 3.3114\n",
+      "404/404 [==============================] - 0s 205us/sample - loss: 10.1839 - mse: 10.1839 - mae: 2.3082 - val_loss: 22.3495 - val_mse: 22.3495 - val_mae: 3.1271\n",
       "Epoch 19/75\n",
-      "404/404 [==============================] - 0s 365us/sample - loss: 10.5854 - mse: 10.5854 - mae: 2.3170 - val_loss: 26.1450 - val_mse: 26.1450 - val_mae: 3.3971\n",
+      "404/404 [==============================] - 0s 188us/sample - loss: 10.1981 - mse: 10.1981 - mae: 2.3012 - val_loss: 21.7859 - val_mse: 21.7859 - val_mae: 3.0725\n",
       "Epoch 20/75\n",
-      "404/404 [==============================] - 0s 401us/sample - loss: 10.2546 - mse: 10.2546 - mae: 2.2813 - val_loss: 26.5278 - val_mse: 26.5278 - val_mae: 3.4465\n",
+      "404/404 [==============================] - 0s 196us/sample - loss: 10.0764 - mse: 10.0764 - mae: 2.3071 - val_loss: 22.4350 - val_mse: 22.4350 - val_mae: 3.1306\n",
       "Epoch 21/75\n",
-      "404/404 [==============================] - 0s 380us/sample - loss: 10.1321 - mse: 10.1321 - mae: 2.2866 - val_loss: 24.0363 - val_mse: 24.0363 - val_mae: 3.2792\n",
+      "404/404 [==============================] - 0s 186us/sample - loss: 9.8176 - mse: 9.8176 - mae: 2.2677 - val_loss: 22.5454 - val_mse: 22.5454 - val_mae: 3.1113\n",
       "Epoch 22/75\n",
-      "404/404 [==============================] - 0s 421us/sample - loss: 9.9169 - mse: 9.9169 - mae: 2.2907 - val_loss: 23.7310 - val_mse: 23.7310 - val_mae: 3.2334\n",
+      "404/404 [==============================] - 0s 176us/sample - loss: 9.5058 - mse: 9.5058 - mae: 2.2135 - val_loss: 22.1667 - val_mse: 22.1667 - val_mae: 3.1033\n",
       "Epoch 23/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 9.6588 - mse: 9.6588 - mae: 2.2284 - val_loss: 23.6472 - val_mse: 23.6472 - val_mae: 3.2013\n",
+      "404/404 [==============================] - 0s 193us/sample - loss: 9.7305 - mse: 9.7305 - mae: 2.2555 - val_loss: 21.7514 - val_mse: 21.7514 - val_mae: 3.1240\n",
       "Epoch 24/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 9.6887 - mse: 9.6887 - mae: 2.2468 - val_loss: 23.5379 - val_mse: 23.5379 - val_mae: 3.1921\n",
+      "404/404 [==============================] - 0s 166us/sample - loss: 9.1781 - mse: 9.1781 - mae: 2.1692 - val_loss: 23.1678 - val_mse: 23.1678 - val_mae: 3.1427\n",
       "Epoch 25/75\n",
-      "404/404 [==============================] - 0s 373us/sample - loss: 9.4049 - mse: 9.4049 - mae: 2.1999 - val_loss: 23.7713 - val_mse: 23.7713 - val_mae: 3.2273\n",
+      "404/404 [==============================] - 0s 163us/sample - loss: 9.6184 - mse: 9.6184 - mae: 2.2674 - val_loss: 22.8964 - val_mse: 22.8964 - val_mae: 3.0964\n",
       "Epoch 26/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 9.2304 - mse: 9.2304 - mae: 2.1946 - val_loss: 23.5093 - val_mse: 23.5093 - val_mae: 3.2072\n",
+      "404/404 [==============================] - 0s 166us/sample - loss: 9.3282 - mse: 9.3282 - mae: 2.2679 - val_loss: 22.6074 - val_mse: 22.6074 - val_mae: 3.0230\n",
       "Epoch 27/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 9.0493 - mse: 9.0493 - mae: 2.1528 - val_loss: 23.7969 - val_mse: 23.7969 - val_mae: 3.2005\n",
+      "404/404 [==============================] - 0s 193us/sample - loss: 8.9916 - mse: 8.9916 - mae: 2.1584 - val_loss: 22.2104 - val_mse: 22.2104 - val_mae: 2.9841\n",
       "Epoch 28/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 8.9363 - mse: 8.9363 - mae: 2.1475 - val_loss: 22.1030 - val_mse: 22.1030 - val_mae: 3.0707\n",
+      "404/404 [==============================] - 0s 163us/sample - loss: 9.0885 - mse: 9.0885 - mae: 2.1703 - val_loss: 21.7229 - val_mse: 21.7229 - val_mae: 2.9437\n",
       "Epoch 29/75\n",
-      "404/404 [==============================] - 0s 373us/sample - loss: 8.7834 - mse: 8.7834 - mae: 2.1231 - val_loss: 22.5153 - val_mse: 22.5153 - val_mae: 3.1532\n",
+      "404/404 [==============================] - 0s 193us/sample - loss: 8.7736 - mse: 8.7736 - mae: 2.1361 - val_loss: 21.7585 - val_mse: 21.7585 - val_mae: 2.9630\n",
       "Epoch 30/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 8.7925 - mse: 8.7925 - mae: 2.1531 - val_loss: 22.0449 - val_mse: 22.0449 - val_mae: 3.1245\n",
+      "404/404 [==============================] - 0s 171us/sample - loss: 8.5250 - mse: 8.5250 - mae: 2.1284 - val_loss: 20.7452 - val_mse: 20.7452 - val_mae: 2.8865\n",
       "Epoch 31/75\n",
-      "404/404 [==============================] - 0s 374us/sample - loss: 9.1879 - mse: 9.1879 - mae: 2.2029 - val_loss: 22.1780 - val_mse: 22.1780 - val_mae: 3.0623\n",
+      "404/404 [==============================] - 0s 186us/sample - loss: 8.5885 - mse: 8.5885 - mae: 2.1236 - val_loss: 21.4387 - val_mse: 21.4387 - val_mae: 2.9751\n",
       "Epoch 32/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 8.7136 - mse: 8.7136 - mae: 2.1164 - val_loss: 21.9815 - val_mse: 21.9815 - val_mae: 3.0969\n",
+      "404/404 [==============================] - 0s 205us/sample - loss: 8.9257 - mse: 8.9257 - mae: 2.1768 - val_loss: 20.3620 - val_mse: 20.3620 - val_mae: 2.9035\n",
       "Epoch 33/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 8.3018 - mse: 8.3018 - mae: 2.0639 - val_loss: 21.0477 - val_mse: 21.0477 - val_mae: 2.9645\n",
+      "404/404 [==============================] - 0s 208us/sample - loss: 8.3427 - mse: 8.3427 - mae: 2.0975 - val_loss: 20.6248 - val_mse: 20.6248 - val_mae: 3.0153\n",
       "Epoch 34/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 8.4156 - mse: 8.4156 - mae: 2.0970 - val_loss: 22.6659 - val_mse: 22.6659 - val_mae: 3.1235\n",
+      "404/404 [==============================] - 0s 171us/sample - loss: 8.3649 - mse: 8.3649 - mae: 2.0923 - val_loss: 21.8397 - val_mse: 21.8397 - val_mae: 2.9297\n",
       "Epoch 35/75\n",
-      "404/404 [==============================] - 0s 350us/sample - loss: 8.2938 - mse: 8.2938 - mae: 2.0567 - val_loss: 20.9574 - val_mse: 20.9574 - val_mae: 2.9746\n",
+      "404/404 [==============================] - 0s 192us/sample - loss: 8.2720 - mse: 8.2720 - mae: 2.0693 - val_loss: 20.4275 - val_mse: 20.4275 - val_mae: 2.8406\n",
       "Epoch 36/75\n",
-      "404/404 [==============================] - 0s 357us/sample - loss: 8.0515 - mse: 8.0515 - mae: 2.0591 - val_loss: 23.2063 - val_mse: 23.2063 - val_mae: 3.1980\n",
+      "404/404 [==============================] - 0s 205us/sample - loss: 7.9768 - mse: 7.9768 - mae: 2.0537 - val_loss: 20.3918 - val_mse: 20.3918 - val_mae: 2.9546\n",
       "Epoch 37/75\n",
-      "404/404 [==============================] - 0s 381us/sample - loss: 8.1403 - mse: 8.1403 - mae: 2.0584 - val_loss: 24.5238 - val_mse: 24.5237 - val_mae: 3.3531\n",
+      "404/404 [==============================] - 0s 191us/sample - loss: 8.0589 - mse: 8.0589 - mae: 2.0246 - val_loss: 20.1589 - val_mse: 20.1589 - val_mae: 2.8728\n",
       "Epoch 38/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 8.0043 - mse: 8.0043 - mae: 2.0776 - val_loss: 22.5424 - val_mse: 22.5424 - val_mae: 3.1494\n",
+      "404/404 [==============================] - 0s 208us/sample - loss: 7.9601 - mse: 7.9601 - mae: 2.0366 - val_loss: 20.2157 - val_mse: 20.2157 - val_mae: 2.9148\n",
       "Epoch 39/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 8.1182 - mse: 8.1182 - mae: 2.0683 - val_loss: 19.7576 - val_mse: 19.7576 - val_mae: 2.8799\n",
+      "404/404 [==============================] - 0s 181us/sample - loss: 7.8259 - mse: 7.8259 - mae: 2.0006 - val_loss: 23.3224 - val_mse: 23.3224 - val_mae: 3.1506\n",
       "Epoch 40/75\n",
-      "404/404 [==============================] - 0s 374us/sample - loss: 7.8578 - mse: 7.8578 - mae: 2.0131 - val_loss: 20.7728 - val_mse: 20.7728 - val_mae: 2.9499\n",
+      "404/404 [==============================] - 0s 156us/sample - loss: 7.9496 - mse: 7.9496 - mae: 2.0141 - val_loss: 21.8039 - val_mse: 21.8039 - val_mae: 3.0086\n",
       "Epoch 41/75\n",
-      "404/404 [==============================] - 0s 382us/sample - loss: 7.5711 - mse: 7.5711 - mae: 1.9896 - val_loss: 20.6170 - val_mse: 20.6170 - val_mae: 2.9936\n",
+      "404/404 [==============================] - 0s 163us/sample - loss: 7.7354 - mse: 7.7354 - mae: 1.9866 - val_loss: 20.1835 - val_mse: 20.1835 - val_mae: 2.8014\n",
       "Epoch 42/75\n",
-      "404/404 [==============================] - 0s 385us/sample - loss: 7.5822 - mse: 7.5822 - mae: 1.9683 - val_loss: 20.8541 - val_mse: 20.8541 - val_mae: 3.0054\n",
+      "404/404 [==============================] - 0s 183us/sample - loss: 7.5411 - mse: 7.5411 - mae: 1.9787 - val_loss: 18.9576 - val_mse: 18.9576 - val_mae: 2.7796\n",
       "Epoch 43/75\n",
-      "404/404 [==============================] - 0s 408us/sample - loss: 7.4533 - mse: 7.4533 - mae: 1.9645 - val_loss: 20.4473 - val_mse: 20.4473 - val_mae: 2.8861\n",
+      "404/404 [==============================] - 0s 181us/sample - loss: 7.4747 - mse: 7.4747 - mae: 1.9731 - val_loss: 19.2336 - val_mse: 19.2336 - val_mae: 2.7739\n",
       "Epoch 44/75\n",
-      "404/404 [==============================] - 0s 396us/sample - loss: 7.5226 - mse: 7.5226 - mae: 1.9509 - val_loss: 20.5193 - val_mse: 20.5193 - val_mae: 2.9619\n",
+      "404/404 [==============================] - 0s 192us/sample - loss: 7.6702 - mse: 7.6702 - mae: 2.0161 - val_loss: 19.8419 - val_mse: 19.8419 - val_mae: 2.8727\n",
       "Epoch 45/75\n",
-      "404/404 [==============================] - 0s 355us/sample - loss: 7.2819 - mse: 7.2819 - mae: 1.9350 - val_loss: 21.4862 - val_mse: 21.4862 - val_mae: 2.9908\n",
+      "404/404 [==============================] - 0s 191us/sample - loss: 7.2534 - mse: 7.2534 - mae: 1.9483 - val_loss: 19.9128 - val_mse: 19.9128 - val_mae: 2.7807\n",
       "Epoch 46/75\n",
-      "404/404 [==============================] - 0s 354us/sample - loss: 7.0130 - mse: 7.0130 - mae: 1.9152 - val_loss: 20.1577 - val_mse: 20.1577 - val_mae: 2.9370\n",
+      "404/404 [==============================] - 0s 208us/sample - loss: 7.6709 - mse: 7.6709 - mae: 1.9986 - val_loss: 20.4885 - val_mse: 20.4885 - val_mae: 2.8457\n",
       "Epoch 47/75\n",
-      "404/404 [==============================] - 0s 375us/sample - loss: 6.9431 - mse: 6.9431 - mae: 1.8819 - val_loss: 21.1210 - val_mse: 21.1210 - val_mae: 2.9746\n",
+      "404/404 [==============================] - 0s 161us/sample - loss: 7.7226 - mse: 7.7226 - mae: 2.0018 - val_loss: 21.0101 - val_mse: 21.0101 - val_mae: 2.9781\n",
       "Epoch 48/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 6.8982 - mse: 6.8982 - mae: 1.9037 - val_loss: 19.2999 - val_mse: 19.2999 - val_mae: 2.8638\n",
+      "404/404 [==============================] - 0s 166us/sample - loss: 7.1772 - mse: 7.1772 - mae: 1.9287 - val_loss: 17.5907 - val_mse: 17.5907 - val_mae: 2.6643\n",
       "Epoch 49/75\n",
-      "404/404 [==============================] - 0s 368us/sample - loss: 6.9521 - mse: 6.9521 - mae: 1.8862 - val_loss: 20.7825 - val_mse: 20.7825 - val_mae: 2.9369\n",
+      "404/404 [==============================] - 0s 168us/sample - loss: 7.1118 - mse: 7.1118 - mae: 1.9456 - val_loss: 19.3413 - val_mse: 19.3413 - val_mae: 2.7788\n",
       "Epoch 50/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 6.8718 - mse: 6.8718 - mae: 1.8889 - val_loss: 20.0288 - val_mse: 20.0288 - val_mae: 2.8915\n",
+      "404/404 [==============================] - 0s 186us/sample - loss: 7.1140 - mse: 7.1140 - mae: 1.9249 - val_loss: 20.5066 - val_mse: 20.5066 - val_mae: 2.8709\n",
       "Epoch 51/75\n",
-      "404/404 [==============================] - 0s 354us/sample - loss: 6.7111 - mse: 6.7111 - mae: 1.8702 - val_loss: 20.4913 - val_mse: 20.4913 - val_mae: 3.0116\n",
+      "404/404 [==============================] - 0s 166us/sample - loss: 7.0934 - mse: 7.0934 - mae: 1.9026 - val_loss: 20.7540 - val_mse: 20.7540 - val_mae: 2.8470\n",
       "Epoch 52/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 6.7492 - mse: 6.7492 - mae: 1.8482 - val_loss: 18.3008 - val_mse: 18.3008 - val_mae: 2.7362\n",
+      "404/404 [==============================] - 0s 183us/sample - loss: 7.0207 - mse: 7.0207 - mae: 1.9068 - val_loss: 19.3910 - val_mse: 19.3910 - val_mae: 2.8267\n",
       "Epoch 53/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 6.6262 - mse: 6.6262 - mae: 1.8395 - val_loss: 18.1885 - val_mse: 18.1885 - val_mae: 2.6920\n",
+      "404/404 [==============================] - 0s 188us/sample - loss: 6.7728 - mse: 6.7728 - mae: 1.8708 - val_loss: 17.6867 - val_mse: 17.6867 - val_mae: 2.6692\n",
       "Epoch 54/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 6.7148 - mse: 6.7148 - mae: 1.8611 - val_loss: 18.5764 - val_mse: 18.5764 - val_mae: 2.6977\n",
+      "404/404 [==============================] - 0s 181us/sample - loss: 6.7816 - mse: 6.7816 - mae: 1.8590 - val_loss: 18.8862 - val_mse: 18.8862 - val_mae: 2.8051\n",
       "Epoch 55/75\n",
-      "404/404 [==============================] - 0s 358us/sample - loss: 6.5425 - mse: 6.5425 - mae: 1.8522 - val_loss: 19.5772 - val_mse: 19.5772 - val_mae: 2.8326\n",
+      "404/404 [==============================] - 0s 167us/sample - loss: 6.7900 - mse: 6.7900 - mae: 1.8735 - val_loss: 16.7665 - val_mse: 16.7665 - val_mae: 2.5738\n",
       "Epoch 56/75\n",
-      "404/404 [==============================] - 0s 423us/sample - loss: 6.3349 - mse: 6.3349 - mae: 1.8175 - val_loss: 19.0932 - val_mse: 19.0932 - val_mae: 2.8260\n",
+      "404/404 [==============================] - 0s 177us/sample - loss: 6.7023 - mse: 6.7023 - mae: 1.8747 - val_loss: 17.3174 - val_mse: 17.3174 - val_mae: 2.7120\n",
       "Epoch 57/75\n",
-      "404/404 [==============================] - 0s 375us/sample - loss: 6.4253 - mse: 6.4253 - mae: 1.7972 - val_loss: 20.4036 - val_mse: 20.4036 - val_mae: 2.9258\n",
+      "404/404 [==============================] - 0s 178us/sample - loss: 6.7268 - mse: 6.7268 - mae: 1.8431 - val_loss: 17.6435 - val_mse: 17.6435 - val_mae: 2.6461\n",
       "Epoch 58/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 6.2897 - mse: 6.2897 - mae: 1.7785 - val_loss: 21.2845 - val_mse: 21.2845 - val_mae: 3.0715\n",
+      "404/404 [==============================] - 0s 181us/sample - loss: 7.0046 - mse: 7.0046 - mae: 1.8949 - val_loss: 18.6629 - val_mse: 18.6629 - val_mae: 2.7825\n",
       "Epoch 59/75\n",
-      "404/404 [==============================] - 0s 378us/sample - loss: 6.7839 - mse: 6.7839 - mae: 1.9027 - val_loss: 18.6853 - val_mse: 18.6853 - val_mae: 2.7709\n",
+      "404/404 [==============================] - 0s 200us/sample - loss: 6.5757 - mse: 6.5757 - mae: 1.8210 - val_loss: 17.8825 - val_mse: 17.8826 - val_mae: 2.7372\n",
       "Epoch 60/75\n",
-      "404/404 [==============================] - 0s 395us/sample - loss: 6.7178 - mse: 6.7178 - mae: 1.8871 - val_loss: 19.5394 - val_mse: 19.5394 - val_mae: 2.8101\n",
+      "404/404 [==============================] - 0s 208us/sample - loss: 6.3822 - mse: 6.3822 - mae: 1.8398 - val_loss: 16.8547 - val_mse: 16.8547 - val_mae: 2.6563\n",
       "Epoch 61/75\n",
-      "404/404 [==============================] - 0s 366us/sample - loss: 6.4152 - mse: 6.4152 - mae: 1.8175 - val_loss: 18.2377 - val_mse: 18.2377 - val_mae: 2.7450\n",
+      "404/404 [==============================] - 0s 193us/sample - loss: 6.4296 - mse: 6.4296 - mae: 1.8330 - val_loss: 16.8793 - val_mse: 16.8793 - val_mae: 2.6211\n",
       "Epoch 62/75\n",
-      "404/404 [==============================] - 0s 384us/sample - loss: 5.9727 - mse: 5.9727 - mae: 1.7630 - val_loss: 19.0252 - val_mse: 19.0252 - val_mae: 2.7960\n",
+      "404/404 [==============================] - 0s 193us/sample - loss: 6.5165 - mse: 6.5165 - mae: 1.8523 - val_loss: 17.5839 - val_mse: 17.5839 - val_mae: 2.7375\n",
       "Epoch 63/75\n",
-      "404/404 [==============================] - 0s 380us/sample - loss: 6.0973 - mse: 6.0973 - mae: 1.8071 - val_loss: 18.8069 - val_mse: 18.8069 - val_mae: 2.8894\n",
+      "404/404 [==============================] - 0s 193us/sample - loss: 6.3401 - mse: 6.3401 - mae: 1.7765 - val_loss: 18.1548 - val_mse: 18.1548 - val_mae: 2.7188\n",
       "Epoch 64/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 6.1074 - mse: 6.1074 - mae: 1.7978 - val_loss: 18.4702 - val_mse: 18.4702 - val_mae: 2.7851\n",
+      "404/404 [==============================] - 0s 183us/sample - loss: 6.5050 - mse: 6.5050 - mae: 1.8493 - val_loss: 17.6989 - val_mse: 17.6989 - val_mae: 2.7358\n",
       "Epoch 65/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 5.9329 - mse: 5.9329 - mae: 1.7545 - val_loss: 18.5321 - val_mse: 18.5321 - val_mae: 2.7933\n",
+      "404/404 [==============================] - 0s 188us/sample - loss: 6.3717 - mse: 6.3717 - mae: 1.8249 - val_loss: 16.0391 - val_mse: 16.0391 - val_mae: 2.5582\n",
       "Epoch 66/75\n",
-      "404/404 [==============================] - 0s 341us/sample - loss: 5.7473 - mse: 5.7473 - mae: 1.7211 - val_loss: 18.5536 - val_mse: 18.5536 - val_mae: 2.8010\n",
+      "404/404 [==============================] - 0s 186us/sample - loss: 6.1185 - mse: 6.1185 - mae: 1.7783 - val_loss: 17.0709 - val_mse: 17.0709 - val_mae: 2.6681\n",
       "Epoch 67/75\n",
-      "404/404 [==============================] - 0s 339us/sample - loss: 5.8866 - mse: 5.8866 - mae: 1.7224 - val_loss: 18.0067 - val_mse: 18.0067 - val_mae: 2.7054\n",
+      "404/404 [==============================] - 0s 163us/sample - loss: 6.0491 - mse: 6.0491 - mae: 1.7822 - val_loss: 18.1096 - val_mse: 18.1096 - val_mae: 2.8730\n",
       "Epoch 68/75\n",
-      "404/404 [==============================] - 0s 337us/sample - loss: 5.7885 - mse: 5.7885 - mae: 1.7391 - val_loss: 17.5502 - val_mse: 17.5502 - val_mae: 2.6767\n",
+      "404/404 [==============================] - 0s 166us/sample - loss: 6.2606 - mse: 6.2606 - mae: 1.8134 - val_loss: 16.9922 - val_mse: 16.9922 - val_mae: 2.5745\n",
       "Epoch 69/75\n",
-      "404/404 [==============================] - 0s 331us/sample - loss: 5.8809 - mse: 5.8809 - mae: 1.7542 - val_loss: 17.0280 - val_mse: 17.0280 - val_mae: 2.6404\n",
+      "404/404 [==============================] - 0s 208us/sample - loss: 5.9244 - mse: 5.9244 - mae: 1.7502 - val_loss: 16.4381 - val_mse: 16.4381 - val_mae: 2.5935\n",
       "Epoch 70/75\n",
-      "404/404 [==============================] - 0s 343us/sample - loss: 5.6028 - mse: 5.6028 - mae: 1.6972 - val_loss: 17.7188 - val_mse: 17.7188 - val_mae: 2.6979\n",
+      "404/404 [==============================] - 0s 211us/sample - loss: 5.8084 - mse: 5.8084 - mae: 1.7379 - val_loss: 17.5640 - val_mse: 17.5640 - val_mae: 2.7420\n",
       "Epoch 71/75\n",
-      "404/404 [==============================] - 0s 337us/sample - loss: 5.4361 - mse: 5.4361 - mae: 1.6741 - val_loss: 16.8852 - val_mse: 16.8852 - val_mae: 2.6126\n",
+      "404/404 [==============================] - 0s 193us/sample - loss: 5.7143 - mse: 5.7143 - mae: 1.7584 - val_loss: 15.7228 - val_mse: 15.7228 - val_mae: 2.5526\n",
       "Epoch 72/75\n",
-      "404/404 [==============================] - 0s 345us/sample - loss: 5.5608 - mse: 5.5608 - mae: 1.7252 - val_loss: 16.7483 - val_mse: 16.7483 - val_mae: 2.6063\n",
+      "404/404 [==============================] - 0s 230us/sample - loss: 5.7376 - mse: 5.7376 - mae: 1.7330 - val_loss: 17.4855 - val_mse: 17.4855 - val_mae: 2.7066\n",
       "Epoch 73/75\n",
-      "404/404 [==============================] - 0s 341us/sample - loss: 5.5022 - mse: 5.5022 - mae: 1.6912 - val_loss: 17.6786 - val_mse: 17.6786 - val_mae: 2.7316\n",
+      "404/404 [==============================] - 0s 168us/sample - loss: 5.8848 - mse: 5.8848 - mae: 1.7319 - val_loss: 16.5765 - val_mse: 16.5765 - val_mae: 2.6397\n",
       "Epoch 74/75\n",
-      "404/404 [==============================] - 0s 396us/sample - loss: 5.2794 - mse: 5.2794 - mae: 1.6478 - val_loss: 17.6115 - val_mse: 17.6115 - val_mae: 2.6773\n",
+      "404/404 [==============================] - 0s 196us/sample - loss: 5.4709 - mse: 5.4709 - mae: 1.7052 - val_loss: 15.8135 - val_mse: 15.8135 - val_mae: 2.6220\n",
       "Epoch 75/75\n",
-      "404/404 [==============================] - 0s 338us/sample - loss: 5.4796 - mse: 5.4796 - mae: 1.6876 - val_loss: 17.2835 - val_mse: 17.2835 - val_mae: 2.7126\n"
+      "404/404 [==============================] - 0s 153us/sample - loss: 5.4749 - mse: 5.4749 - mae: 1.6632 - val_loss: 15.5588 - val_mse: 15.5588 - val_mae: 2.4818\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7f36340c6b38>"
+       "<tensorflow.python.keras.callbacks.History at 0x1a1bf538388>"
       ]
      },
-     "execution_count": 3,
+     "execution_count": 5,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -449,7 +449,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 6,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -460,25 +460,17 @@
     "outputId": "ae996575-78e2-43fb-9dbe-5d44aaf0b430"
    },
    "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
-      "  warnings.warn(CV_WARNING, FutureWarning)\n"
-     ]
-    },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Best: 0.65234375 using {'batch_size': 10, 'epochs': 20}\n",
-      "Means: 0.65234375, Stdev: 0.033298728782667764 with: {'batch_size': 10, 'epochs': 20}\n",
-      "Means: 0.6263020833333334, Stdev: 0.01813592223591682 with: {'batch_size': 20, 'epochs': 20}\n",
-      "Means: 0.6041666666666666, Stdev: 0.037782859709757574 with: {'batch_size': 40, 'epochs': 20}\n",
-      "Means: 0.5533854166666666, Stdev: 0.03210632293213009 with: {'batch_size': 60, 'epochs': 20}\n",
-      "Means: 0.61328125, Stdev: 0.024079742199097563 with: {'batch_size': 80, 'epochs': 20}\n",
-      "Means: 0.5611979166666666, Stdev: 0.038450060052691144 with: {'batch_size': 100, 'epochs': 20}\n"
+      "Best: 0.6615482568740845 using {'batch_size': 10, 'epochs': 20}\n",
+      "Means: 0.6615482568740845, Stdev: 0.04387017394776617 with: {'batch_size': 10, 'epochs': 20}\n",
+      "Means: 0.6471946358680725, Stdev: 0.050710719436528676 with: {'batch_size': 20, 'epochs': 20}\n",
+      "Means: 0.6211951494216919, Stdev: 0.054220960333549034 with: {'batch_size': 40, 'epochs': 20}\n",
+      "Means: 0.5717341542243958, Stdev: 0.06440496888757774 with: {'batch_size': 60, 'epochs': 20}\n",
+      "Means: 0.5624819695949554, Stdev: 0.04522985434079598 with: {'batch_size': 80, 'epochs': 20}\n",
+      "Means: 0.576844084262848, Stdev: 0.05859901121845465 with: {'batch_size': 100, 'epochs': 20}\n"
      ]
     }
    ],
@@ -551,7 +543,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 7,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -566,11 +558,11 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Best: 0.7044270833333334 using {'batch_size': 20, 'epochs': 200}\n",
-      "Means: 0.6666666666666666, Stdev: 0.028940248399600087 with: {'batch_size': 20, 'epochs': 20}\n",
-      "Means: 0.6588541666666666, Stdev: 0.028940248399600087 with: {'batch_size': 20, 'epochs': 40}\n",
-      "Means: 0.6848958333333334, Stdev: 0.03498705427745938 with: {'batch_size': 20, 'epochs': 60}\n",
-      "Means: 0.7044270833333334, Stdev: 0.018414239093399672 with: {'batch_size': 20, 'epochs': 200}\n"
+      "Best: 0.7110092639923096 using {'batch_size': 20, 'epochs': 200}\n",
+      "Means: 0.6315508127212525, Stdev: 0.03260988444862231 with: {'batch_size': 20, 'epochs': 20}\n",
+      "Means: 0.6810457587242127, Stdev: 0.03757470277183249 with: {'batch_size': 20, 'epochs': 40}\n",
+      "Means: 0.6797470569610595, Stdev: 0.030080889566468257 with: {'batch_size': 20, 'epochs': 60}\n",
+      "Means: 0.7110092639923096, Stdev: 0.055126734664131816 with: {'batch_size': 20, 'epochs': 200}\n"
      ]
     }
    ],
@@ -730,36 +722,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 8,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/html": [
-       "\n",
-       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
-       "                Project page: <a href=\"https://app.wandb.ai/lambda-ds7/boston\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro</a><br/>\n",
-       "            "
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/plain": [
-       "W&B Run: https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro"
-      ]
-     },
-     "execution_count": 6,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "outputs": [],
    "source": [
     "import wandb\n",
     "from wandb.keras import WandbCallback"
@@ -767,7 +732,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 9,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -778,13 +743,34 @@
     "outputId": "b05e251e-508f-46e6-865b-f869ae2a5dc4"
    },
    "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "wandb: ERROR Not authenticated.  Copy a key from https://app.wandb.ai/authorize\n"
+     ]
+    },
+    {
+     "name": "stdin",
+     "output_type": "stream",
+     "text": [
+      "API Key:  ········································\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\dylan/.netrc\n"
+     ]
+    },
     {
      "data": {
       "text/html": [
        "\n",
        "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
        "                Project page: <a href=\"https://app.wandb.ai/lambda-ds7/boston\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/kkgdtc31\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/kkgdtc31</a><br/>\n",
+       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/omjjlg6l\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/omjjlg6l</a><br/>\n",
        "            "
       ],
       "text/plain": [
@@ -800,114 +786,114 @@
      "text": [
       "Train on 270 samples, validate on 134 samples\n",
       "Epoch 1/50\n",
-      "270/270 [==============================] - 1s 3ms/sample - loss: 492.3539 - mse: 492.3539 - mae: 20.3197 - val_loss: 481.5445 - val_mse: 481.5445 - val_mae: 19.6138\n",
+      "270/270 [==============================] - 1s 3ms/sample - loss: 497.4602 - mse: 497.4601 - mae: 20.4472 - val_loss: 487.0721 - val_mse: 487.0721 - val_mae: 19.6643\n",
       "Epoch 2/50\n",
-      "270/270 [==============================] - 0s 591us/sample - loss: 239.4999 - mse: 239.4999 - mae: 12.8064 - val_loss: 113.8561 - val_mse: 113.8561 - val_mae: 8.2962\n",
+      "270/270 [==============================] - 0s 450us/sample - loss: 241.8103 - mse: 241.8103 - mae: 12.8978 - val_loss: 130.1979 - val_mse: 130.1979 - val_mae: 8.5996\n",
       "Epoch 3/50\n",
-      "270/270 [==============================] - 0s 618us/sample - loss: 56.2921 - mse: 56.2921 - mae: 5.8988 - val_loss: 62.7912 - val_mse: 62.7912 - val_mae: 5.6465\n",
+      "270/270 [==============================] - 0s 341us/sample - loss: 70.3606 - mse: 70.3606 - mae: 6.0239 - val_loss: 69.9884 - val_mse: 69.9884 - val_mae: 5.9542\n",
       "Epoch 4/50\n",
-      "270/270 [==============================] - 0s 613us/sample - loss: 29.4994 - mse: 29.4994 - mae: 3.9653 - val_loss: 37.9256 - val_mse: 37.9256 - val_mae: 4.1730\n",
+      "270/270 [==============================] - 0s 360us/sample - loss: 32.3231 - mse: 32.3231 - mae: 4.0425 - val_loss: 40.9746 - val_mse: 40.9746 - val_mae: 4.2716\n",
       "Epoch 5/50\n",
-      "270/270 [==============================] - 0s 608us/sample - loss: 20.6919 - mse: 20.6919 - mae: 3.3022 - val_loss: 31.7489 - val_mse: 31.7489 - val_mae: 3.7113\n",
+      "270/270 [==============================] - 0s 354us/sample - loss: 22.1177 - mse: 22.1177 - mae: 3.4000 - val_loss: 33.7252 - val_mse: 33.7252 - val_mae: 3.6896\n",
       "Epoch 6/50\n",
-      "270/270 [==============================] - 0s 602us/sample - loss: 17.2701 - mse: 17.2701 - mae: 3.0291 - val_loss: 27.3921 - val_mse: 27.3921 - val_mae: 3.4958\n",
+      "270/270 [==============================] - 0s 358us/sample - loss: 18.0847 - mse: 18.0847 - mae: 2.9902 - val_loss: 28.8280 - val_mse: 28.8280 - val_mae: 3.4667\n",
       "Epoch 7/50\n",
-      "270/270 [==============================] - 0s 671us/sample - loss: 15.5172 - mse: 15.5172 - mae: 2.8537 - val_loss: 25.3208 - val_mse: 25.3208 - val_mae: 3.3650\n",
+      "270/270 [==============================] - 0s 379us/sample - loss: 15.6052 - mse: 15.6052 - mae: 2.8136 - val_loss: 25.3167 - val_mse: 25.3167 - val_mae: 3.3166\n",
       "Epoch 8/50\n",
-      "270/270 [==============================] - 0s 661us/sample - loss: 13.7548 - mse: 13.7548 - mae: 2.7089 - val_loss: 23.8920 - val_mse: 23.8920 - val_mae: 3.2746\n",
+      "270/270 [==============================] - 0s 370us/sample - loss: 14.3842 - mse: 14.3842 - mae: 2.7163 - val_loss: 24.1151 - val_mse: 24.1151 - val_mae: 3.3243\n",
       "Epoch 9/50\n",
-      "270/270 [==============================] - 0s 606us/sample - loss: 12.3745 - mse: 12.3745 - mae: 2.5662 - val_loss: 22.1294 - val_mse: 22.1294 - val_mae: 3.1509\n",
+      "270/270 [==============================] - 0s 401us/sample - loss: 13.0730 - mse: 13.0730 - mae: 2.5770 - val_loss: 21.8467 - val_mse: 21.8467 - val_mae: 3.1278\n",
       "Epoch 10/50\n",
-      "270/270 [==============================] - 0s 614us/sample - loss: 11.2424 - mse: 11.2424 - mae: 2.4804 - val_loss: 20.5718 - val_mse: 20.5718 - val_mae: 3.0461\n",
+      "270/270 [==============================] - 0s 291us/sample - loss: 12.0784 - mse: 12.0784 - mae: 2.5499 - val_loss: 22.4376 - val_mse: 22.4376 - val_mae: 3.0754\n",
       "Epoch 11/50\n",
-      "270/270 [==============================] - 0s 605us/sample - loss: 10.6098 - mse: 10.6098 - mae: 2.4178 - val_loss: 20.3467 - val_mse: 20.3467 - val_mae: 3.0251\n",
+      "270/270 [==============================] - 0s 685us/sample - loss: 10.8180 - mse: 10.8180 - mae: 2.3366 - val_loss: 20.0038 - val_mse: 20.0038 - val_mae: 2.9319\n",
       "Epoch 12/50\n",
-      "270/270 [==============================] - 0s 576us/sample - loss: 10.0011 - mse: 10.0011 - mae: 2.3257 - val_loss: 18.4283 - val_mse: 18.4283 - val_mae: 2.8938\n",
+      "270/270 [==============================] - 0s 504us/sample - loss: 10.0688 - mse: 10.0688 - mae: 2.3476 - val_loss: 18.7184 - val_mse: 18.7184 - val_mae: 2.8589\n",
       "Epoch 13/50\n",
-      "270/270 [==============================] - 0s 666us/sample - loss: 9.1287 - mse: 9.1287 - mae: 2.2384 - val_loss: 18.2024 - val_mse: 18.2024 - val_mae: 2.9116\n",
+      "270/270 [==============================] - 0s 393us/sample - loss: 9.6005 - mse: 9.6005 - mae: 2.2659 - val_loss: 19.1303 - val_mse: 19.1303 - val_mae: 2.9314\n",
       "Epoch 14/50\n",
-      "270/270 [==============================] - 0s 603us/sample - loss: 8.6211 - mse: 8.6211 - mae: 2.1980 - val_loss: 17.4749 - val_mse: 17.4749 - val_mae: 2.8290\n",
+      "270/270 [==============================] - 0s 400us/sample - loss: 9.0055 - mse: 9.0055 - mae: 2.1855 - val_loss: 18.2156 - val_mse: 18.2156 - val_mae: 2.8230\n",
       "Epoch 15/50\n",
-      "270/270 [==============================] - 0s 463us/sample - loss: 8.4558 - mse: 8.4558 - mae: 2.2087 - val_loss: 17.7878 - val_mse: 17.7878 - val_mae: 2.8516\n",
+      "270/270 [==============================] - 0s 367us/sample - loss: 8.7344 - mse: 8.7344 - mae: 2.2046 - val_loss: 17.3841 - val_mse: 17.3841 - val_mae: 2.7751\n",
       "Epoch 16/50\n",
-      "270/270 [==============================] - 0s 626us/sample - loss: 8.3626 - mse: 8.3626 - mae: 2.2031 - val_loss: 16.7101 - val_mse: 16.7101 - val_mae: 2.7820\n",
+      "270/270 [==============================] - 0s 348us/sample - loss: 8.4589 - mse: 8.4589 - mae: 2.1575 - val_loss: 17.3587 - val_mse: 17.3587 - val_mae: 2.7665\n",
       "Epoch 17/50\n",
-      "270/270 [==============================] - 0s 607us/sample - loss: 7.9180 - mse: 7.9180 - mae: 2.1265 - val_loss: 16.6064 - val_mse: 16.6064 - val_mae: 2.7419\n",
+      "270/270 [==============================] - 0s 356us/sample - loss: 7.7988 - mse: 7.7988 - mae: 2.0247 - val_loss: 16.5562 - val_mse: 16.5562 - val_mae: 2.7318\n",
       "Epoch 18/50\n",
-      "270/270 [==============================] - 0s 479us/sample - loss: 7.5552 - mse: 7.5552 - mae: 2.0235 - val_loss: 17.2872 - val_mse: 17.2872 - val_mae: 2.8539\n",
+      "270/270 [==============================] - 0s 230us/sample - loss: 7.8235 - mse: 7.8235 - mae: 2.0618 - val_loss: 16.6231 - val_mse: 16.6231 - val_mae: 2.7288\n",
       "Epoch 19/50\n",
-      "270/270 [==============================] - 0s 616us/sample - loss: 7.0971 - mse: 7.0971 - mae: 2.0038 - val_loss: 16.5110 - val_mse: 16.5110 - val_mae: 2.8042\n",
+      "270/270 [==============================] - 0s 267us/sample - loss: 7.3982 - mse: 7.3982 - mae: 2.0321 - val_loss: 16.8338 - val_mse: 16.8338 - val_mae: 2.7616\n",
       "Epoch 20/50\n",
-      "270/270 [==============================] - 0s 606us/sample - loss: 6.7068 - mse: 6.7068 - mae: 1.9539 - val_loss: 15.5886 - val_mse: 15.5886 - val_mae: 2.7048\n",
+      "270/270 [==============================] - 0s 367us/sample - loss: 7.1294 - mse: 7.1294 - mae: 1.9499 - val_loss: 16.4422 - val_mse: 16.4422 - val_mae: 2.7226\n",
       "Epoch 21/50\n",
-      "270/270 [==============================] - 0s 461us/sample - loss: 6.8542 - mse: 6.8542 - mae: 1.9979 - val_loss: 17.2378 - val_mse: 17.2378 - val_mae: 2.8853\n",
+      "270/270 [==============================] - 0s 400us/sample - loss: 7.2728 - mse: 7.2728 - mae: 1.9622 - val_loss: 16.0603 - val_mse: 16.0603 - val_mae: 2.6857\n",
       "Epoch 22/50\n",
-      "270/270 [==============================] - 0s 474us/sample - loss: 6.5719 - mse: 6.5719 - mae: 1.9312 - val_loss: 16.3043 - val_mse: 16.3043 - val_mae: 2.7756\n",
+      "270/270 [==============================] - 0s 381us/sample - loss: 7.0448 - mse: 7.0448 - mae: 1.9544 - val_loss: 15.5995 - val_mse: 15.5995 - val_mae: 2.6190\n",
       "Epoch 23/50\n",
-      "270/270 [==============================] - 0s 478us/sample - loss: 6.6161 - mse: 6.6161 - mae: 1.9572 - val_loss: 15.7992 - val_mse: 15.7992 - val_mae: 2.7219\n",
+      "270/270 [==============================] - 0s 419us/sample - loss: 6.8751 - mse: 6.8751 - mae: 1.9358 - val_loss: 15.5665 - val_mse: 15.5665 - val_mae: 2.6463\n",
       "Epoch 24/50\n",
-      "270/270 [==============================] - 0s 491us/sample - loss: 7.1269 - mse: 7.1269 - mae: 2.0137 - val_loss: 16.5402 - val_mse: 16.5402 - val_mae: 2.8005\n",
+      "270/270 [==============================] - 0s 393us/sample - loss: 6.4780 - mse: 6.4780 - mae: 1.8987 - val_loss: 15.5413 - val_mse: 15.5413 - val_mae: 2.6312\n",
       "Epoch 25/50\n",
-      "270/270 [==============================] - 0s 479us/sample - loss: 6.3382 - mse: 6.3382 - mae: 1.8540 - val_loss: 16.5034 - val_mse: 16.5034 - val_mae: 2.7864\n",
+      "270/270 [==============================] - 0s 400us/sample - loss: 6.2271 - mse: 6.2271 - mae: 1.8346 - val_loss: 15.3083 - val_mse: 15.3083 - val_mae: 2.5833\n",
       "Epoch 26/50\n",
-      "270/270 [==============================] - 0s 488us/sample - loss: 5.9442 - mse: 5.9442 - mae: 1.8251 - val_loss: 15.6558 - val_mse: 15.6558 - val_mae: 2.7102\n",
+      "270/270 [==============================] - 0s 393us/sample - loss: 6.1794 - mse: 6.1794 - mae: 1.8433 - val_loss: 15.4447 - val_mse: 15.4447 - val_mae: 2.6174\n",
       "Epoch 27/50\n",
-      "270/270 [==============================] - 0s 604us/sample - loss: 5.5832 - mse: 5.5832 - mae: 1.7432 - val_loss: 15.3021 - val_mse: 15.3021 - val_mae: 2.6862\n",
+      "270/270 [==============================] - 0s 333us/sample - loss: 5.8716 - mse: 5.8716 - mae: 1.7857 - val_loss: 15.5975 - val_mse: 15.5975 - val_mae: 2.6337\n",
       "Epoch 28/50\n",
-      "270/270 [==============================] - 0s 436us/sample - loss: 5.4530 - mse: 5.4530 - mae: 1.7354 - val_loss: 15.4570 - val_mse: 15.4570 - val_mae: 2.6846\n",
+      "270/270 [==============================] - 0s 322us/sample - loss: 5.6593 - mse: 5.6593 - mae: 1.7398 - val_loss: 15.3668 - val_mse: 15.3668 - val_mae: 2.6088\n",
       "Epoch 29/50\n",
-      "270/270 [==============================] - 0s 441us/sample - loss: 5.3070 - mse: 5.3070 - mae: 1.7079 - val_loss: 15.8510 - val_mse: 15.8510 - val_mae: 2.7644\n",
+      "270/270 [==============================] - 0s 433us/sample - loss: 5.4996 - mse: 5.4996 - mae: 1.7288 - val_loss: 14.9277 - val_mse: 14.9277 - val_mae: 2.5519\n",
       "Epoch 30/50\n",
-      "270/270 [==============================] - 0s 477us/sample - loss: 5.4157 - mse: 5.4157 - mae: 1.7321 - val_loss: 15.9160 - val_mse: 15.9160 - val_mae: 2.7134\n",
+      "270/270 [==============================] - 0s 241us/sample - loss: 5.4775 - mse: 5.4775 - mae: 1.7398 - val_loss: 15.2106 - val_mse: 15.2106 - val_mae: 2.6224\n",
       "Epoch 31/50\n",
-      "270/270 [==============================] - 0s 452us/sample - loss: 5.2639 - mse: 5.2639 - mae: 1.6981 - val_loss: 15.3554 - val_mse: 15.3554 - val_mae: 2.6662\n",
+      "270/270 [==============================] - 0s 241us/sample - loss: 5.3648 - mse: 5.3648 - mae: 1.6902 - val_loss: 15.3627 - val_mse: 15.3627 - val_mae: 2.6183\n",
       "Epoch 32/50\n",
-      "270/270 [==============================] - 0s 475us/sample - loss: 5.7687 - mse: 5.7687 - mae: 1.8045 - val_loss: 15.7151 - val_mse: 15.7151 - val_mae: 2.6867\n",
+      "270/270 [==============================] - 0s 222us/sample - loss: 5.2574 - mse: 5.2574 - mae: 1.6917 - val_loss: 15.2822 - val_mse: 15.2822 - val_mae: 2.6032\n",
       "Epoch 33/50\n",
-      "270/270 [==============================] - 0s 462us/sample - loss: 5.5210 - mse: 5.5210 - mae: 1.7367 - val_loss: 15.4227 - val_mse: 15.4227 - val_mae: 2.6561\n",
+      "270/270 [==============================] - 0s 267us/sample - loss: 5.1731 - mse: 5.1731 - mae: 1.6928 - val_loss: 15.1216 - val_mse: 15.1216 - val_mae: 2.5810\n",
       "Epoch 34/50\n",
-      "270/270 [==============================] - 0s 474us/sample - loss: 5.5663 - mse: 5.5663 - mae: 1.7294 - val_loss: 15.3376 - val_mse: 15.3376 - val_mae: 2.6991\n",
+      "270/270 [==============================] - 0s 293us/sample - loss: 5.3405 - mse: 5.3405 - mae: 1.7183 - val_loss: 15.4217 - val_mse: 15.4217 - val_mae: 2.6179\n",
       "Epoch 35/50\n",
-      "270/270 [==============================] - 0s 626us/sample - loss: 5.0063 - mse: 5.0063 - mae: 1.6196 - val_loss: 15.2642 - val_mse: 15.2642 - val_mae: 2.6796\n",
+      "270/270 [==============================] - 0s 348us/sample - loss: 4.7843 - mse: 4.7843 - mae: 1.6137 - val_loss: 14.9215 - val_mse: 14.9215 - val_mae: 2.5581\n",
       "Epoch 36/50\n",
-      "270/270 [==============================] - 0s 459us/sample - loss: 4.7251 - mse: 4.7251 - mae: 1.5727 - val_loss: 15.4858 - val_mse: 15.4858 - val_mae: 2.7288\n",
+      "270/270 [==============================] - 0s 226us/sample - loss: 4.8130 - mse: 4.8130 - mae: 1.6637 - val_loss: 15.4531 - val_mse: 15.4531 - val_mae: 2.6464\n",
       "Epoch 37/50\n",
-      "270/270 [==============================] - 0s 604us/sample - loss: 4.6394 - mse: 4.6394 - mae: 1.5854 - val_loss: 15.1139 - val_mse: 15.1139 - val_mae: 2.6305\n",
+      "270/270 [==============================] - 0s 215us/sample - loss: 5.0687 - mse: 5.0687 - mae: 1.6986 - val_loss: 15.1843 - val_mse: 15.1843 - val_mae: 2.5829\n",
       "Epoch 38/50\n",
-      "270/270 [==============================] - 0s 592us/sample - loss: 4.5669 - mse: 4.5669 - mae: 1.5548 - val_loss: 14.9898 - val_mse: 14.9898 - val_mae: 2.6340\n",
+      "270/270 [==============================] - 0s 293us/sample - loss: 4.6484 - mse: 4.6484 - mae: 1.5797 - val_loss: 14.8381 - val_mse: 14.8381 - val_mae: 2.5342\n",
       "Epoch 39/50\n",
-      "270/270 [==============================] - 0s 458us/sample - loss: 4.4480 - mse: 4.4480 - mae: 1.5334 - val_loss: 15.6389 - val_mse: 15.6389 - val_mae: 2.7337\n",
+      "270/270 [==============================] - 0s 237us/sample - loss: 4.5749 - mse: 4.5749 - mae: 1.5782 - val_loss: 15.3553 - val_mse: 15.3553 - val_mae: 2.6104\n",
       "Epoch 40/50\n",
-      "270/270 [==============================] - 0s 455us/sample - loss: 4.4119 - mse: 4.4119 - mae: 1.5426 - val_loss: 15.0723 - val_mse: 15.0723 - val_mae: 2.6709\n",
+      "270/270 [==============================] - 0s 356us/sample - loss: 4.7285 - mse: 4.7285 - mae: 1.6363 - val_loss: 15.0948 - val_mse: 15.0948 - val_mae: 2.6001\n",
       "Epoch 41/50\n",
-      "270/270 [==============================] - 0s 473us/sample - loss: 4.0797 - mse: 4.0797 - mae: 1.4725 - val_loss: 15.4706 - val_mse: 15.4706 - val_mae: 2.6707\n",
+      "270/270 [==============================] - 0s 241us/sample - loss: 4.1272 - mse: 4.1272 - mae: 1.5059 - val_loss: 15.0459 - val_mse: 15.0459 - val_mae: 2.5659\n",
       "Epoch 42/50\n",
-      "270/270 [==============================] - 0s 449us/sample - loss: 4.0619 - mse: 4.0619 - mae: 1.4692 - val_loss: 15.2423 - val_mse: 15.2423 - val_mae: 2.6165\n",
+      "270/270 [==============================] - 0s 315us/sample - loss: 4.2764 - mse: 4.2764 - mae: 1.5450 - val_loss: 14.3775 - val_mse: 14.3775 - val_mae: 2.4936\n",
       "Epoch 43/50\n",
-      "270/270 [==============================] - 0s 465us/sample - loss: 4.1861 - mse: 4.1861 - mae: 1.5076 - val_loss: 15.7510 - val_mse: 15.7510 - val_mae: 2.7279\n",
+      "270/270 [==============================] - 0s 293us/sample - loss: 3.9798 - mse: 3.9798 - mae: 1.5065 - val_loss: 14.8734 - val_mse: 14.8734 - val_mae: 2.5631\n",
       "Epoch 44/50\n",
-      "270/270 [==============================] - 0s 462us/sample - loss: 4.1128 - mse: 4.1128 - mae: 1.4810 - val_loss: 15.4814 - val_mse: 15.4814 - val_mae: 2.6562\n",
+      "270/270 [==============================] - 0s 248us/sample - loss: 4.1573 - mse: 4.1573 - mae: 1.5343 - val_loss: 14.7036 - val_mse: 14.7036 - val_mae: 2.5342\n",
       "Epoch 45/50\n",
-      "270/270 [==============================] - 0s 441us/sample - loss: 4.2171 - mse: 4.2171 - mae: 1.5205 - val_loss: 16.3839 - val_mse: 16.3839 - val_mae: 2.8194\n",
+      "270/270 [==============================] - 0s 326us/sample - loss: 4.4996 - mse: 4.4996 - mae: 1.5766 - val_loss: 15.2827 - val_mse: 15.2827 - val_mae: 2.6388\n",
       "Epoch 46/50\n",
-      "270/270 [==============================] - 0s 422us/sample - loss: 4.2609 - mse: 4.2609 - mae: 1.5548 - val_loss: 15.3587 - val_mse: 15.3587 - val_mae: 2.7161\n",
+      "270/270 [==============================] - 0s 252us/sample - loss: 4.4517 - mse: 4.4517 - mae: 1.5820 - val_loss: 14.9738 - val_mse: 14.9738 - val_mae: 2.5931\n",
       "Epoch 47/50\n",
-      "270/270 [==============================] - 0s 454us/sample - loss: 4.4635 - mse: 4.4635 - mae: 1.5440 - val_loss: 15.7736 - val_mse: 15.7736 - val_mae: 2.7184\n",
+      "270/270 [==============================] - 0s 341us/sample - loss: 3.9006 - mse: 3.9006 - mae: 1.4795 - val_loss: 14.8107 - val_mse: 14.8107 - val_mae: 2.5413\n",
       "Epoch 48/50\n",
-      "270/270 [==============================] - 0s 426us/sample - loss: 3.7406 - mse: 3.7406 - mae: 1.4147 - val_loss: 15.6718 - val_mse: 15.6718 - val_mae: 2.7468\n",
+      "270/270 [==============================] - 0s 219us/sample - loss: 4.0402 - mse: 4.0402 - mae: 1.5145 - val_loss: 15.3612 - val_mse: 15.3612 - val_mae: 2.6034\n",
       "Epoch 49/50\n",
-      "270/270 [==============================] - 0s 445us/sample - loss: 3.6173 - mse: 3.6173 - mae: 1.3816 - val_loss: 15.7291 - val_mse: 15.7291 - val_mae: 2.7789\n",
+      "270/270 [==============================] - 0s 226us/sample - loss: 3.6662 - mse: 3.6662 - mae: 1.4471 - val_loss: 14.4810 - val_mse: 14.4810 - val_mae: 2.5095\n",
       "Epoch 50/50\n",
-      "270/270 [==============================] - 0s 430us/sample - loss: 3.6303 - mse: 3.6303 - mae: 1.4266 - val_loss: 15.4937 - val_mse: 15.4937 - val_mae: 2.7390\n"
+      "270/270 [==============================] - 0s 244us/sample - loss: 3.7150 - mse: 3.7150 - mae: 1.4549 - val_loss: 14.7753 - val_mse: 14.7753 - val_mae: 2.5391\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7f315c319be0>"
+       "<tensorflow.python.keras.callbacks.History at 0x1a1c96342c8>"
       ]
      },
-     "execution_count": 8,
+     "execution_count": 9,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -975,7 +961,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": 10,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -995,15 +981,15 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 11,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Create sweep with ID: huau0u9r\n",
-      "Sweep URL: https://app.wandb.ai/lambda-ds7/boston/sweeps/huau0u9r\n"
+      "Create sweep with ID: 58gwkoqy\n",
+      "Sweep URL: https://app.wandb.ai/lambda-ds7/boston/sweeps/58gwkoqy\n"
      ]
     }
    ],
@@ -1013,7 +999,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
+   "execution_count": 16,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -1040,6 +1026,8 @@
     "    model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
     "    model.add(Dense(64, activation='relu'))\n",
     "    model.add(Dense(64, activation='relu'))\n",
+    "    model.add(Dense(64, activation='relu'))\n",
+    "    model.add(Dense(64, activation='relu'))\n",
     "    model.add(Dense(1))\n",
     "\n",
     "    # Optimizer \n",
@@ -1066,28 +1054,18 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "wandb: Agent Starting Run: 2g77kp6k with config:\n",
-      "\tbatch_size: 308.503347845309\n",
-      "\tepochs: 704.9395850579006\n",
-      "\tlearning_rate: 1.480005523005428\n",
-      "wandb: Agent Started Run: 2g77kp6k\n"
+      "wandb: Agent Starting Run: ypdqd4oy with config:\n",
+      "\tbatch_size: 142.71598091095586\n",
+      "\tepochs: 196.4830509512626\n",
+      "\tlearning_rate: 0.5329442156477536\n"
      ]
     },
     {
-     "data": {
-      "text/html": [
-       "\n",
-       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
-       "                Project page: <a href=\"https://app.wandb.ai/lambda-ds7/boston\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/t4w9l4ye\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/t4w9l4ye</a><br/>\n",
-       "            "
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "wandb: Ctrl-c pressed. Waiting for runs to end. Press ctrl-c again to terminate them.\n"
+     ]
     }
    ],
    "source": [
@@ -1153,9 +1131,9 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "conda_tensorflow_p36",
+   "display_name": "Python 3",
    "language": "python",
-   "name": "conda_tensorflow_p36"
+   "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
@@ -1167,7 +1145,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.6.5"
+   "version": "3.7.4"
   }
  },
  "nbformat": 4,
diff --git a/requirements.txt b/requirements.txt
index 30dce62..2932bb8 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,54 +1,5 @@
-absl-py==0.7.1
-appnope==0.1.0
-astor==0.8.0
-backcall==0.1.0
-certifi==2019.6.16
-cycler==0.10.0
-decorator==4.4.0
-gast==0.2.2
-google-pasta==0.1.7
-grpcio==1.22.0
-h5py==2.9.0
 ipykernel==5.1.1
-ipython==7.6.1
-ipython-genutils==0.2.0
-jedi==0.13.3
-joblib==0.13.2
-jupyter-client==5.2.4
-jupyter-core==4.5.0
-Keras-Applications==1.0.8
-Keras-Preprocessing==1.1.0
-kiwisolver==1.1.0
-Markdown==3.1.1
-matplotlib==3.1.0
-mkl-fft==1.0.12
-mkl-random==1.0.2
-mkl-service==2.0.2
-numpy==1.16.4
-pandas==0.24.2
-parso==0.5.0
-patsy==0.5.1
-pexpect==4.7.0
-pickleshare==0.7.5
-prompt-toolkit==2.0.9
-protobuf==3.9.0
-ptyprocess==0.6.0
-Pygments==2.4.2
-pyparsing==2.4.0
-python-dateutil==2.8.0
-pytz==2019.1
-pyzmq==18.0.0
-scikit-learn==0.21.2
-scipy==1.2.1
-seaborn==0.9.0
-six==1.12.0
-statsmodels==0.10.0
-tensorboard==1.14.0
-tensorflow==1.14.0
-tensorflow-estimator==1.14.0
-termcolor==1.1.0
-tornado==6.0.3
-traitlets==4.3.2
-wcwidth==0.1.7
-Werkzeug==0.15.4
-wrapt==1.11.2
+pandas==1.0.1
+scikit-learn==0.22
+tensorflow==2.1.0
+wandb==0.8.27
\ No newline at end of file
